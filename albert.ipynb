{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T08:04:44.029507Z",
     "start_time": "2023-07-05T08:04:44.001436Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up GPU for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T08:04:44.029827Z",
     "start_time": "2023-07-05T08:04:44.023311Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T08:04:44.044297Z",
     "start_time": "2023-07-05T08:04:44.038240Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "#     print('mps supported')\n",
    "# else:\n",
    "#     print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T08:04:44.072948Z",
     "start_time": "2023-07-05T08:04:44.044776Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#read file\n",
    "df = pd.read_csv('train.csv')\n",
    "df1 = pd.read_csv('dev.csv')\n",
    "\n",
    "df1 = df1.rename(columns={'labels': 'label'})\n",
    "\n",
    "df = pd.concat([df, df1], ignore_index=True)\n",
    "\n",
    "#select only text, tweet ids, sentiment label and sentiment agree columns\n",
    "df = df[['text','label']]\n",
    "\n",
    "X = df[['text']]\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T08:04:44.079343Z",
     "start_time": "2023-07-05T08:04:44.061325Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8722, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T08:04:44.079809Z",
     "start_time": "2023-07-05T08:04:44.074178Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "LEARNING_RATE = 2e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T08:04:44.083976Z",
     "start_time": "2023-07-05T08:04:44.080355Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "\n",
    "    text = text\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T08:04:44.373387Z",
     "start_time": "2023-07-05T08:04:44.085961Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AlbertTokenizer\n",
    "\n",
    "# Load the BERT tokenizer\n",
    "pretrained_bertmodel = 'albert-base-v2'  # Specify the ALBERT model you want to use\n",
    "tokenizer = AlbertTokenizer.from_pretrained(pretrained_bertmodel)\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(df):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    df (pd.DataFrame): DataFrame containing text1 and text2 columns to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every row in the dataframe...\n",
    "    for index, row in df.iterrows():\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentences\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` tokens to the start and end\n",
    "        #    (3) Truncate/Pad sentences to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention masks\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(row['text']),\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True\n",
    "            )\n",
    "\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T08:04:48.458479Z",
     "start_time": "2023-07-05T08:04:44.449035Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eugene/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_inputs, X_masks = preprocessing_for_bert(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T08:04:48.458758Z",
     "start_time": "2023-07-05T08:04:48.457935Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y=torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BertClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T08:04:48.459006Z",
     "start_time": "2023-07-05T08:04:48.458239Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import AlbertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Binary Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        D_in, H, D_out = 768, 50, 1  # Change D_out to 1 for binary classification\n",
    "\n",
    "        self.bert = AlbertModel.from_pretrained(pretrained_bertmodel)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.35),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "        return logits.squeeze()  # Squeeze the output to remove the extra dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T08:04:48.459263Z",
     "start_time": "2023-07-05T08:04:48.458840Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer, and the learning rate scheduler.\"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = torch.optim.AdamW(bert_classifier.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0,  # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T08:04:48.501482Z",
     "start_time": "2023-07-05T08:04:48.459195Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "threshold = 0.5\n",
    "# loss_fn = nn.CrossEntropyLoss() # No adjust weight\n",
    "# loss_fn = nn.CrossEntropyLoss(weight = torch.tensor(class_weights, dtype=torch.float)) # Adjust weight\n",
    "\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Convert true labels to Float data type\n",
    "            b_labels = b_labels.float()\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Compute logits\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(logits.view(-1), b_labels.view(-1))\n",
    "\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "\n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits.squeeze(), b_labels.float())\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = (logits > threshold).long().flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-05T08:04:48.531318Z",
     "start_time": "2023-07-05T08:04:48.501020Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader, threshold=0.5):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers care disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Apply sigmoid to convert logits to probabilities\n",
    "        probs = logits.cpu()\n",
    "#         probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        all_logits.append(probs)\n",
    "\n",
    "    # Concatenate probabilities from each batch\n",
    "    all_probs = np.concatenate(all_logits, axis=0)\n",
    "\n",
    "    return all_probs\n",
    "\n",
    "# # Compute predicted probabilities on the test set\n",
    "# probs = bert_predict(bert_classifier, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-07-05T08:04:48.527841Z"
    },
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n",
      "Epoch 1\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.293651   |     -      |     -     |   11.44  \n",
      "   1    |   40    |   0.231557   |     -      |     -     |   10.75  \n",
      "   1    |   60    |   0.238886   |     -      |     -     |   10.80  \n",
      "   1    |   80    |   0.257182   |     -      |     -     |   10.84  \n",
      "   1    |   100   |   0.213266   |     -      |     -     |   10.90  \n",
      "   1    |   120   |   0.201072   |     -      |     -     |   10.97  \n",
      "   1    |   140   |   0.190723   |     -      |     -     |   10.92  \n",
      "   1    |   160   |   0.207503   |     -      |     -     |   11.03  \n",
      "   1    |   180   |   0.187047   |     -      |     -     |   11.07  \n",
      "   1    |   200   |   0.200491   |     -      |     -     |   11.13  \n",
      "   1    |   220   |   0.213125   |     -      |     -     |   11.16  \n",
      "   1    |   240   |   0.173647   |     -      |     -     |   11.27  \n",
      "   1    |   260   |   0.178423   |     -      |     -     |   11.59  \n",
      "   1    |   280   |   0.205829   |     -      |     -     |   11.95  \n",
      "   1    |   300   |   0.207542   |     -      |     -     |   12.27  \n",
      "   1    |   320   |   0.180649   |     -      |     -     |   12.53  \n",
      "   1    |   340   |   0.190833   |     -      |     -     |   12.68  \n",
      "   1    |   360   |   0.190497   |     -      |     -     |   12.77  \n",
      "   1    |   380   |   0.184936   |     -      |     -     |   12.99  \n",
      "   1    |   400   |   0.229942   |     -      |     -     |   13.04  \n",
      "   1    |   420   |   0.192552   |     -      |     -     |   13.27  \n",
      "   1    |   440   |   0.201471   |     -      |     -     |   13.58  \n",
      "   1    |   460   |   0.188697   |     -      |     -     |   13.59  \n",
      "   1    |   480   |   0.193623   |     -      |     -     |   13.81  \n",
      "   1    |   490   |   0.162200   |     -      |     -     |   6.70   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.205659   |  0.179074  |   73.32   |  307.44  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 2\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.173113   |     -      |     -     |   15.42  \n",
      "   1    |   40    |   0.190127   |     -      |     -     |   14.59  \n",
      "   1    |   60    |   0.152237   |     -      |     -     |   14.62  \n",
      "   1    |   80    |   0.146653   |     -      |     -     |   14.73  \n",
      "   1    |   100   |   0.167363   |     -      |     -     |   14.68  \n",
      "   1    |   120   |   0.154692   |     -      |     -     |   14.68  \n",
      "   1    |   140   |   0.164376   |     -      |     -     |   14.99  \n",
      "   1    |   160   |   0.176768   |     -      |     -     |   15.34  \n",
      "   1    |   180   |   0.177162   |     -      |     -     |   14.96  \n",
      "   1    |   200   |   0.159778   |     -      |     -     |   14.99  \n",
      "   1    |   220   |   0.155865   |     -      |     -     |   14.93  \n",
      "   1    |   240   |   0.126580   |     -      |     -     |   14.95  \n",
      "   1    |   260   |   0.176310   |     -      |     -     |   15.31  \n",
      "   1    |   280   |   0.154369   |     -      |     -     |   15.64  \n",
      "   1    |   300   |   0.158439   |     -      |     -     |   15.60  \n",
      "   1    |   320   |   0.171407   |     -      |     -     |   15.64  \n",
      "   1    |   340   |   0.159234   |     -      |     -     |   15.65  \n",
      "   1    |   360   |   0.143680   |     -      |     -     |   15.06  \n",
      "   1    |   380   |   0.167133   |     -      |     -     |   15.04  \n",
      "   1    |   400   |   0.173853   |     -      |     -     |   15.09  \n",
      "   1    |   420   |   0.134547   |     -      |     -     |   15.27  \n",
      "   1    |   440   |   0.152530   |     -      |     -     |   15.29  \n",
      "   1    |   460   |   0.161949   |     -      |     -     |   16.29  \n",
      "   1    |   480   |   0.185851   |     -      |     -     |   16.27  \n",
      "   1    |   490   |   0.154673   |     -      |     -     |   7.78   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.161711   |  0.172781  |   74.91   |  390.42  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 3\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.140074   |     -      |     -     |   17.09  \n",
      "   1    |   40    |   0.134272   |     -      |     -     |   16.36  \n",
      "   1    |   60    |   0.129044   |     -      |     -     |   16.38  \n",
      "   1    |   80    |   0.112032   |     -      |     -     |   16.35  \n",
      "   1    |   100   |   0.147745   |     -      |     -     |   15.54  \n",
      "   1    |   120   |   0.135648   |     -      |     -     |   15.51  \n",
      "   1    |   140   |   0.137828   |     -      |     -     |   15.54  \n",
      "   1    |   160   |   0.150850   |     -      |     -     |   15.57  \n",
      "   1    |   180   |   0.131646   |     -      |     -     |   15.64  \n",
      "   1    |   200   |   0.159256   |     -      |     -     |   15.64  \n",
      "   1    |   220   |   0.123590   |     -      |     -     |   16.70  \n",
      "   1    |   240   |   0.145383   |     -      |     -     |   16.94  \n",
      "   1    |   260   |   0.162818   |     -      |     -     |   15.88  \n",
      "   1    |   280   |   0.135921   |     -      |     -     |   15.46  \n",
      "   1    |   300   |   0.138144   |     -      |     -     |   15.46  \n",
      "   1    |   320   |   0.143330   |     -      |     -     |   16.45  \n",
      "   1    |   340   |   0.148448   |     -      |     -     |   16.48  \n",
      "   1    |   360   |   0.126501   |     -      |     -     |   16.48  \n",
      "   1    |   380   |   0.134862   |     -      |     -     |   16.49  \n",
      "   1    |   400   |   0.136120   |     -      |     -     |   15.82  \n",
      "   1    |   420   |   0.149230   |     -      |     -     |   15.69  \n",
      "   1    |   440   |   0.136139   |     -      |     -     |   15.67  \n",
      "   1    |   460   |   0.133439   |     -      |     -     |   15.70  \n",
      "   1    |   480   |   0.150777   |     -      |     -     |   15.89  \n",
      "   1    |   490   |   0.119602   |     -      |     -     |   7.87   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.138896   |  0.172781  |   74.91   |  410.46  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Fold 1\n",
      "Epoch 1\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.294516   |     -      |     -     |   17.60  \n",
      "   1    |   40    |   0.247864   |     -      |     -     |   16.87  \n",
      "   1    |   60    |   0.222082   |     -      |     -     |   16.81  \n",
      "   1    |   80    |   0.235147   |     -      |     -     |   16.21  \n",
      "   1    |   100   |   0.194897   |     -      |     -     |   15.94  \n",
      "   1    |   120   |   0.176856   |     -      |     -     |   15.96  \n",
      "   1    |   140   |   0.228671   |     -      |     -     |   15.93  \n",
      "   1    |   160   |   0.207976   |     -      |     -     |   15.93  \n",
      "   1    |   180   |   0.212329   |     -      |     -     |   15.96  \n",
      "   1    |   200   |   0.203171   |     -      |     -     |   16.00  \n",
      "   1    |   220   |   0.210158   |     -      |     -     |   17.77  \n",
      "   1    |   240   |   0.211323   |     -      |     -     |   16.51  \n",
      "   1    |   260   |   0.192078   |     -      |     -     |   15.95  \n",
      "   1    |   280   |   0.192784   |     -      |     -     |   15.96  \n",
      "   1    |   300   |   0.205518   |     -      |     -     |   15.92  \n",
      "   1    |   320   |   0.183255   |     -      |     -     |   15.94  \n",
      "   1    |   340   |   0.180988   |     -      |     -     |   15.95  \n",
      "   1    |   360   |   0.198508   |     -      |     -     |   15.97  \n",
      "   1    |   380   |   0.198391   |     -      |     -     |   15.92  \n",
      "   1    |   400   |   0.192433   |     -      |     -     |   15.96  \n",
      "   1    |   420   |   0.188262   |     -      |     -     |   16.46  \n",
      "   1    |   440   |   0.173353   |     -      |     -     |   16.65  \n",
      "   1    |   460   |   0.163869   |     -      |     -     |   16.65  \n",
      "   1    |   480   |   0.168871   |     -      |     -     |   16.46  \n",
      "   1    |   490   |   0.199990   |     -      |     -     |   7.57   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.203585   |  0.170882  |   73.36   |  415.64  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 2\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.171947   |     -      |     -     |   18.24  \n",
      "   1    |   40    |   0.182393   |     -      |     -     |   17.36  \n",
      "   1    |   60    |   0.163826   |     -      |     -     |   16.48  \n",
      "   1    |   80    |   0.164974   |     -      |     -     |   16.37  \n",
      "   1    |   100   |   0.155764   |     -      |     -     |   16.35  \n",
      "   1    |   120   |   0.170561   |     -      |     -     |   16.40  \n",
      "   1    |   140   |   0.171646   |     -      |     -     |   16.40  \n",
      "   1    |   160   |   0.155885   |     -      |     -     |   16.41  \n",
      "   1    |   180   |   0.173869   |     -      |     -     |   16.38  \n",
      "   1    |   200   |   0.156679   |     -      |     -     |   16.35  \n",
      "   1    |   220   |   0.155665   |     -      |     -     |   16.35  \n",
      "   1    |   240   |   0.143804   |     -      |     -     |   16.38  \n",
      "   1    |   260   |   0.166566   |     -      |     -     |   16.35  \n",
      "   1    |   280   |   0.154537   |     -      |     -     |   15.63  \n",
      "   1    |   300   |   0.160791   |     -      |     -     |   15.60  \n",
      "   1    |   320   |   0.141806   |     -      |     -     |   15.62  \n",
      "   1    |   340   |   0.156300   |     -      |     -     |   17.05  \n",
      "   1    |   360   |   0.174584   |     -      |     -     |   16.75  \n",
      "   1    |   380   |   0.168904   |     -      |     -     |   16.75  \n",
      "   1    |   400   |   0.156643   |     -      |     -     |   16.05  \n",
      "   1    |   420   |   0.165936   |     -      |     -     |   15.91  \n",
      "   1    |   440   |   0.161762   |     -      |     -     |   15.91  \n",
      "   1    |   460   |   0.130095   |     -      |     -     |   15.94  \n",
      "   1    |   480   |   0.147986   |     -      |     -     |   15.91  \n",
      "   1    |   490   |   0.148892   |     -      |     -     |   7.62   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.160325   |  0.173236  |   73.88   |  417.94  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 3\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.137584   |     -      |     -     |   17.84  \n",
      "   1    |   40    |   0.142025   |     -      |     -     |   16.98  \n",
      "   1    |   60    |   0.137532   |     -      |     -     |   16.30  \n",
      "   1    |   80    |   0.142499   |     -      |     -     |   16.04  \n",
      "   1    |   100   |   0.155719   |     -      |     -     |   16.05  \n",
      "   1    |   120   |   0.131537   |     -      |     -     |   16.11  \n",
      "   1    |   140   |   0.129792   |     -      |     -     |   16.13  \n",
      "   1    |   160   |   0.128869   |     -      |     -     |   16.31  \n",
      "   1    |   180   |   0.133660   |     -      |     -     |   17.33  \n",
      "   1    |   200   |   0.128633   |     -      |     -     |   16.59  \n",
      "   1    |   220   |   0.128917   |     -      |     -     |   16.34  \n",
      "   1    |   240   |   0.127209   |     -      |     -     |   15.82  \n",
      "   1    |   260   |   0.133493   |     -      |     -     |   15.83  \n",
      "   1    |   280   |   0.126381   |     -      |     -     |   16.37  \n",
      "   1    |   300   |   0.131596   |     -      |     -     |   16.94  \n",
      "   1    |   320   |   0.144624   |     -      |     -     |   16.94  \n",
      "   1    |   340   |   0.120470   |     -      |     -     |   16.11  \n",
      "   1    |   360   |   0.136323   |     -      |     -     |   16.03  \n",
      "   1    |   380   |   0.152550   |     -      |     -     |   16.03  \n",
      "   1    |   400   |   0.148533   |     -      |     -     |   16.05  \n",
      "   1    |   420   |   0.142511   |     -      |     -     |   16.10  \n",
      "   1    |   440   |   0.148274   |     -      |     -     |   16.08  \n",
      "   1    |   460   |   0.129541   |     -      |     -     |   15.91  \n",
      "   1    |   480   |   0.140419   |     -      |     -     |   17.27  \n",
      "   1    |   490   |   0.133896   |     -      |     -     |   8.37   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.136559   |  0.173236  |   73.88   |  421.69  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Fold 2\n",
      "Epoch 1\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.320314   |     -      |     -     |   15.84  \n",
      "   1    |   40    |   0.217469   |     -      |     -     |   16.00  \n",
      "   1    |   60    |   0.241416   |     -      |     -     |   16.76  \n",
      "   1    |   80    |   0.248063   |     -      |     -     |   16.73  \n",
      "   1    |   100   |   0.198161   |     -      |     -     |   15.90  \n",
      "   1    |   120   |   0.216245   |     -      |     -     |   15.89  \n",
      "   1    |   140   |   0.222736   |     -      |     -     |   15.93  \n",
      "   1    |   160   |   0.231275   |     -      |     -     |   15.88  \n",
      "   1    |   180   |   0.296873   |     -      |     -     |   15.86  \n",
      "   1    |   200   |   0.252353   |     -      |     -     |   18.37  \n",
      "   1    |   220   |   0.216742   |     -      |     -     |   16.53  \n",
      "   1    |   240   |   0.219365   |     -      |     -     |   16.22  \n",
      "   1    |   260   |   0.227505   |     -      |     -     |   16.20  \n",
      "   1    |   280   |   0.221636   |     -      |     -     |   16.20  \n",
      "   1    |   300   |   0.204524   |     -      |     -     |   16.16  \n",
      "   1    |   320   |   0.220162   |     -      |     -     |   16.23  \n",
      "   1    |   340   |   0.204090   |     -      |     -     |   16.25  \n",
      "   1    |   360   |   0.177292   |     -      |     -     |   16.18  \n",
      "   1    |   380   |   0.198605   |     -      |     -     |   16.25  \n",
      "   1    |   400   |   0.195714   |     -      |     -     |   16.20  \n",
      "   1    |   420   |   0.208964   |     -      |     -     |   16.23  \n",
      "   1    |   440   |   0.166178   |     -      |     -     |   16.19  \n",
      "   1    |   460   |   0.180603   |     -      |     -     |   16.18  \n",
      "   1    |   480   |   0.189408   |     -      |     -     |   16.20  \n",
      "   1    |   490   |   0.215508   |     -      |     -     |   7.82   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.219937   |  0.183830  |   70.57   |  415.59  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 2\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.176186   |     -      |     -     |   18.55  \n",
      "   1    |   40    |   0.178257   |     -      |     -     |   16.97  \n",
      "   1    |   60    |   0.173051   |     -      |     -     |   16.14  \n",
      "   1    |   80    |   0.168210   |     -      |     -     |   15.79  \n",
      "   1    |   100   |   0.176563   |     -      |     -     |   16.46  \n",
      "   1    |   120   |   0.183251   |     -      |     -     |   17.48  \n",
      "   1    |   140   |   0.148652   |     -      |     -     |   16.90  \n",
      "   1    |   160   |   0.160586   |     -      |     -     |   16.07  \n",
      "   1    |   180   |   0.177218   |     -      |     -     |   16.03  \n",
      "   1    |   200   |   0.154073   |     -      |     -     |   16.05  \n",
      "   1    |   220   |   0.163445   |     -      |     -     |   16.05  \n",
      "   1    |   240   |   0.178445   |     -      |     -     |   15.99  \n",
      "   1    |   260   |   0.194592   |     -      |     -     |   16.06  \n",
      "   1    |   280   |   0.174926   |     -      |     -     |   16.10  \n",
      "   1    |   300   |   0.163912   |     -      |     -     |   16.07  \n",
      "   1    |   320   |   0.159177   |     -      |     -     |   16.05  \n",
      "   1    |   340   |   0.180163   |     -      |     -     |   16.86  \n",
      "   1    |   360   |   0.165506   |     -      |     -     |   17.30  \n",
      "   1    |   380   |   0.160300   |     -      |     -     |   15.61  \n",
      "   1    |   400   |   0.141278   |     -      |     -     |   15.67  \n",
      "   1    |   420   |   0.162381   |     -      |     -     |   17.19  \n",
      "   1    |   440   |   0.170979   |     -      |     -     |   17.40  \n",
      "   1    |   460   |   0.154579   |     -      |     -     |   15.69  \n",
      "   1    |   480   |   0.162308   |     -      |     -     |   15.71  \n",
      "   1    |   490   |   0.160816   |     -      |     -     |   8.73   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.167709   |  0.157476  |   78.18   |  423.28  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 3\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.146749   |     -      |     -     |   16.81  \n",
      "   1    |   40    |   0.137869   |     -      |     -     |   15.93  \n",
      "   1    |   60    |   0.166044   |     -      |     -     |   15.99  \n",
      "   1    |   80    |   0.143412   |     -      |     -     |   16.14  \n",
      "   1    |   100   |   0.147509   |     -      |     -     |   16.09  \n",
      "   1    |   120   |   0.133161   |     -      |     -     |   16.23  \n",
      "   1    |   140   |   0.158115   |     -      |     -     |   16.24  \n",
      "   1    |   160   |   0.167730   |     -      |     -     |   16.23  \n",
      "   1    |   180   |   0.143235   |     -      |     -     |   16.28  \n",
      "   1    |   200   |   0.146178   |     -      |     -     |   16.24  \n",
      "   1    |   220   |   0.140905   |     -      |     -     |   16.30  \n",
      "   1    |   240   |   0.160556   |     -      |     -     |   16.31  \n",
      "   1    |   260   |   0.157778   |     -      |     -     |   16.34  \n",
      "   1    |   280   |   0.146136   |     -      |     -     |   16.23  \n",
      "   1    |   300   |   0.153167   |     -      |     -     |   16.32  \n",
      "   1    |   320   |   0.152814   |     -      |     -     |   16.29  \n",
      "   1    |   340   |   0.131963   |     -      |     -     |   16.29  \n",
      "   1    |   360   |   0.144949   |     -      |     -     |   16.29  \n",
      "   1    |   380   |   0.147272   |     -      |     -     |   16.32  \n",
      "   1    |   400   |   0.137851   |     -      |     -     |   16.35  \n",
      "   1    |   420   |   0.144067   |     -      |     -     |   16.30  \n",
      "   1    |   440   |   0.139345   |     -      |     -     |   16.30  \n",
      "   1    |   460   |   0.154597   |     -      |     -     |   16.33  \n",
      "   1    |   480   |   0.132944   |     -      |     -     |   16.25  \n",
      "   1    |   490   |   0.139152   |     -      |     -     |   7.73   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.147098   |  0.157476  |   78.18   |  415.43  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Fold 3\n",
      "Epoch 1\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.300553   |     -      |     -     |   16.61  \n",
      "   1    |   40    |   0.244924   |     -      |     -     |   15.83  \n",
      "   1    |   60    |   0.231730   |     -      |     -     |   15.83  \n",
      "   1    |   80    |   0.218614   |     -      |     -     |   15.84  \n",
      "   1    |   100   |   0.243839   |     -      |     -     |   16.90  \n",
      "   1    |   120   |   0.198908   |     -      |     -     |   16.89  \n",
      "   1    |   140   |   0.212683   |     -      |     -     |   16.85  \n",
      "   1    |   160   |   0.199809   |     -      |     -     |   16.41  \n",
      "   1    |   180   |   0.185894   |     -      |     -     |   15.95  \n",
      "   1    |   200   |   0.182119   |     -      |     -     |   15.96  \n",
      "   1    |   220   |   0.204528   |     -      |     -     |   15.93  \n",
      "   1    |   240   |   0.191822   |     -      |     -     |   15.99  \n",
      "   1    |   260   |   0.192691   |     -      |     -     |   16.75  \n",
      "   1    |   280   |   0.197790   |     -      |     -     |   17.72  \n",
      "   1    |   300   |   0.211162   |     -      |     -     |   16.92  \n",
      "   1    |   320   |   0.197591   |     -      |     -     |   16.20  \n",
      "   1    |   340   |   0.160781   |     -      |     -     |   15.94  \n",
      "   1    |   360   |   0.181453   |     -      |     -     |   15.99  \n",
      "   1    |   380   |   0.189638   |     -      |     -     |   15.98  \n",
      "   1    |   400   |   0.163387   |     -      |     -     |   16.03  \n",
      "   1    |   420   |   0.187393   |     -      |     -     |   15.99  \n",
      "   1    |   440   |   0.184497   |     -      |     -     |   16.01  \n",
      "   1    |   460   |   0.169052   |     -      |     -     |   16.00  \n",
      "   1    |   480   |   0.168387   |     -      |     -     |   16.00  \n",
      "   1    |   490   |   0.140577   |     -      |     -     |   8.26   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.199778   |  0.179515  |   73.86   |  417.70  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 2\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.160446   |     -      |     -     |   17.90  \n",
      "   1    |   40    |   0.140926   |     -      |     -     |   16.85  \n",
      "   1    |   60    |   0.174543   |     -      |     -     |   16.11  \n",
      "   1    |   80    |   0.168345   |     -      |     -     |   16.13  \n",
      "   1    |   100   |   0.134213   |     -      |     -     |   16.13  \n",
      "   1    |   120   |   0.162859   |     -      |     -     |   16.16  \n",
      "   1    |   140   |   0.151464   |     -      |     -     |   16.14  \n",
      "   1    |   160   |   0.166239   |     -      |     -     |   16.14  \n",
      "   1    |   180   |   0.144901   |     -      |     -     |   16.19  \n",
      "   1    |   200   |   0.153417   |     -      |     -     |   16.13  \n",
      "   1    |   220   |   0.173651   |     -      |     -     |   16.05  \n",
      "   1    |   240   |   0.155743   |     -      |     -     |   16.05  \n",
      "   1    |   260   |   0.167465   |     -      |     -     |   16.16  \n",
      "   1    |   280   |   0.154054   |     -      |     -     |   16.18  \n",
      "   1    |   300   |   0.161521   |     -      |     -     |   16.07  \n",
      "   1    |   320   |   0.153944   |     -      |     -     |   17.13  \n",
      "   1    |   340   |   0.150245   |     -      |     -     |   17.78  \n",
      "   1    |   360   |   0.156953   |     -      |     -     |   15.67  \n",
      "   1    |   380   |   0.147070   |     -      |     -     |   15.69  \n",
      "   1    |   400   |   0.164068   |     -      |     -     |   16.93  \n",
      "   1    |   420   |   0.150290   |     -      |     -     |   17.09  \n",
      "   1    |   440   |   0.152981   |     -      |     -     |   16.36  \n",
      "   1    |   460   |   0.132417   |     -      |     -     |   16.30  \n",
      "   1    |   480   |   0.146903   |     -      |     -     |   16.35  \n",
      "   1    |   490   |   0.163481   |     -      |     -     |   7.90   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.155374   |  0.174453  |   74.66   |  419.23  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 3\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.104409   |     -      |     -     |   17.56  \n",
      "   1    |   40    |   0.132070   |     -      |     -     |   16.85  \n",
      "   1    |   60    |   0.134600   |     -      |     -     |   16.85  \n",
      "   1    |   80    |   0.139400   |     -      |     -     |   16.83  \n",
      "   1    |   100   |   0.133839   |     -      |     -     |   16.81  \n",
      "   1    |   120   |   0.137987   |     -      |     -     |   16.63  \n",
      "   1    |   140   |   0.126300   |     -      |     -     |   15.98  \n",
      "   1    |   160   |   0.121783   |     -      |     -     |   15.98  \n",
      "   1    |   180   |   0.138405   |     -      |     -     |   15.99  \n",
      "   1    |   200   |   0.146120   |     -      |     -     |   15.99  \n",
      "   1    |   220   |   0.145008   |     -      |     -     |   17.14  \n",
      "   1    |   240   |   0.146082   |     -      |     -     |   17.17  \n",
      "   1    |   260   |   0.134550   |     -      |     -     |   16.89  \n",
      "   1    |   280   |   0.133796   |     -      |     -     |   16.24  \n",
      "   1    |   300   |   0.122693   |     -      |     -     |   16.27  \n",
      "   1    |   320   |   0.150639   |     -      |     -     |   16.30  \n",
      "   1    |   340   |   0.117203   |     -      |     -     |   16.27  \n",
      "   1    |   360   |   0.135678   |     -      |     -     |   16.25  \n",
      "   1    |   380   |   0.136384   |     -      |     -     |   16.25  \n",
      "   1    |   400   |   0.119982   |     -      |     -     |   16.28  \n",
      "   1    |   420   |   0.139298   |     -      |     -     |   16.24  \n",
      "   1    |   440   |   0.137842   |     -      |     -     |   16.28  \n",
      "   1    |   460   |   0.122061   |     -      |     -     |   16.24  \n",
      "   1    |   480   |   0.117627   |     -      |     -     |   16.27  \n",
      "   1    |   490   |   0.134316   |     -      |     -     |   7.82   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.132225   |  0.174453  |   74.66   |  420.89  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Fold 4\n",
      "Epoch 1\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.323527   |     -      |     -     |   18.90  \n",
      "   1    |   40    |   0.234333   |     -      |     -     |   16.58  \n",
      "   1    |   60    |   0.271882   |     -      |     -     |   16.50  \n",
      "   1    |   80    |   0.276973   |     -      |     -     |   16.55  \n",
      "   1    |   100   |   0.249921   |     -      |     -     |   16.60  \n",
      "   1    |   120   |   0.215130   |     -      |     -     |   16.56  \n",
      "   1    |   140   |   0.229342   |     -      |     -     |   16.57  \n",
      "   1    |   160   |   0.242510   |     -      |     -     |   15.90  \n",
      "   1    |   180   |   0.221412   |     -      |     -     |   15.78  \n",
      "   1    |   200   |   0.242806   |     -      |     -     |   15.79  \n",
      "   1    |   220   |   0.196673   |     -      |     -     |   15.82  \n",
      "   1    |   240   |   0.223484   |     -      |     -     |   16.34  \n",
      "   1    |   260   |   0.195710   |     -      |     -     |   17.73  \n",
      "   1    |   280   |   0.207482   |     -      |     -     |   16.80  \n",
      "   1    |   300   |   0.211136   |     -      |     -     |   16.82  \n",
      "   1    |   320   |   0.198056   |     -      |     -     |   16.19  \n",
      "   1    |   340   |   0.226994   |     -      |     -     |   15.43  \n",
      "   1    |   360   |   0.192040   |     -      |     -     |   16.10  \n",
      "   1    |   380   |   0.207019   |     -      |     -     |   16.49  \n",
      "   1    |   400   |   0.182464   |     -      |     -     |   16.49  \n",
      "   1    |   420   |   0.168340   |     -      |     -     |   16.50  \n",
      "   1    |   440   |   0.206460   |     -      |     -     |   16.43  \n",
      "   1    |   460   |   0.192046   |     -      |     -     |   16.41  \n",
      "   1    |   480   |   0.167001   |     -      |     -     |   16.45  \n",
      "   1    |   490   |   0.155973   |     -      |     -     |   7.93   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.219018   |  0.167824  |   76.02   |  421.40  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 2\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.165381   |     -      |     -     |   17.20  \n",
      "   1    |   40    |   0.177419   |     -      |     -     |   16.39  \n",
      "   1    |   60    |   0.143117   |     -      |     -     |   16.37  \n",
      "   1    |   80    |   0.191423   |     -      |     -     |   16.43  \n",
      "   1    |   100   |   0.174046   |     -      |     -     |   16.40  \n",
      "   1    |   120   |   0.190260   |     -      |     -     |   16.43  \n",
      "   1    |   140   |   0.164442   |     -      |     -     |   16.38  \n",
      "   1    |   160   |   0.152026   |     -      |     -     |   16.44  \n",
      "   1    |   180   |   0.183981   |     -      |     -     |   16.41  \n",
      "   1    |   200   |   0.150859   |     -      |     -     |   16.43  \n",
      "   1    |   220   |   0.184429   |     -      |     -     |   16.43  \n",
      "   1    |   240   |   0.164216   |     -      |     -     |   16.43  \n",
      "   1    |   260   |   0.173539   |     -      |     -     |   16.46  \n",
      "   1    |   280   |   0.143615   |     -      |     -     |   16.42  \n",
      "   1    |   300   |   0.134990   |     -      |     -     |   16.42  \n",
      "   1    |   320   |   0.172817   |     -      |     -     |   16.41  \n",
      "   1    |   340   |   0.168698   |     -      |     -     |   16.42  \n",
      "   1    |   360   |   0.179314   |     -      |     -     |   16.41  \n",
      "   1    |   380   |   0.176721   |     -      |     -     |   16.40  \n",
      "   1    |   400   |   0.154444   |     -      |     -     |   16.39  \n",
      "   1    |   420   |   0.148598   |     -      |     -     |   16.36  \n",
      "   1    |   440   |   0.159373   |     -      |     -     |   15.82  \n",
      "   1    |   460   |   0.163044   |     -      |     -     |   15.67  \n",
      "   1    |   480   |   0.155596   |     -      |     -     |   17.71  \n",
      "   1    |   490   |   0.144323   |     -      |     -     |   8.04   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.165083   |  0.159133  |   76.59   |  420.19  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 3\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.114724   |     -      |     -     |   17.12  \n",
      "   1    |   40    |   0.148739   |     -      |     -     |   16.28  \n",
      "   1    |   60    |   0.158582   |     -      |     -     |   16.32  \n",
      "   1    |   80    |   0.142681   |     -      |     -     |   16.35  \n",
      "   1    |   100   |   0.108696   |     -      |     -     |   16.32  \n",
      "   1    |   120   |   0.154304   |     -      |     -     |   16.34  \n",
      "   1    |   140   |   0.129713   |     -      |     -     |   16.29  \n",
      "   1    |   160   |   0.134539   |     -      |     -     |   16.11  \n",
      "   1    |   180   |   0.153687   |     -      |     -     |   16.13  \n",
      "   1    |   200   |   0.130062   |     -      |     -     |   16.99  \n",
      "   1    |   220   |   0.160039   |     -      |     -     |   16.97  \n",
      "   1    |   240   |   0.123044   |     -      |     -     |   16.96  \n",
      "   1    |   260   |   0.134756   |     -      |     -     |   16.94  \n",
      "   1    |   280   |   0.123503   |     -      |     -     |   16.19  \n",
      "   1    |   300   |   0.148328   |     -      |     -     |   16.10  \n",
      "   1    |   320   |   0.146640   |     -      |     -     |   16.11  \n",
      "   1    |   340   |   0.133863   |     -      |     -     |   16.10  \n",
      "   1    |   360   |   0.150122   |     -      |     -     |   16.09  \n",
      "   1    |   380   |   0.153602   |     -      |     -     |   16.07  \n",
      "   1    |   400   |   0.155427   |     -      |     -     |   16.08  \n",
      "   1    |   420   |   0.146165   |     -      |     -     |   16.12  \n",
      "   1    |   440   |   0.134673   |     -      |     -     |   16.06  \n",
      "   1    |   460   |   0.146457   |     -      |     -     |   16.09  \n",
      "   1    |   480   |   0.153642   |     -      |     -     |   17.85  \n",
      "   1    |   490   |   0.151424   |     -      |     -     |   8.59   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.141240   |  0.159133  |   76.59   |  422.37  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Fold 5\n",
      "Epoch 1\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.310048   |     -      |     -     |   17.04  \n",
      "   1    |   40    |   0.240704   |     -      |     -     |   16.82  \n",
      "   1    |   60    |   0.203174   |     -      |     -     |   16.30  \n",
      "   1    |   80    |   0.232484   |     -      |     -     |   15.94  \n",
      "   1    |   100   |   0.189784   |     -      |     -     |   15.93  \n",
      "   1    |   120   |   0.218509   |     -      |     -     |   15.92  \n",
      "   1    |   140   |   0.196442   |     -      |     -     |   16.58  \n",
      "   1    |   160   |   0.208121   |     -      |     -     |   17.95  \n",
      "   1    |   180   |   0.253089   |     -      |     -     |   16.05  \n",
      "   1    |   200   |   0.196912   |     -      |     -     |   15.97  \n",
      "   1    |   220   |   0.214135   |     -      |     -     |   15.98  \n",
      "   1    |   240   |   0.193687   |     -      |     -     |   15.95  \n",
      "   1    |   260   |   0.194951   |     -      |     -     |   15.96  \n",
      "   1    |   280   |   0.225382   |     -      |     -     |   16.09  \n",
      "   1    |   300   |   0.193315   |     -      |     -     |   16.40  \n",
      "   1    |   320   |   0.207857   |     -      |     -     |   16.39  \n",
      "   1    |   340   |   0.175185   |     -      |     -     |   16.39  \n",
      "   1    |   360   |   0.170913   |     -      |     -     |   16.39  \n",
      "   1    |   380   |   0.195020   |     -      |     -     |   16.40  \n",
      "   1    |   400   |   0.188376   |     -      |     -     |   16.37  \n",
      "   1    |   420   |   0.177542   |     -      |     -     |   16.35  \n",
      "   1    |   440   |   0.170594   |     -      |     -     |   16.32  \n",
      "   1    |   460   |   0.190498   |     -      |     -     |   16.34  \n",
      "   1    |   480   |   0.178267   |     -      |     -     |   16.36  \n",
      "   1    |   490   |   0.195213   |     -      |     -     |   7.87   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.205218   |  0.178055  |   73.30   |  417.70  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 2\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.159139   |     -      |     -     |   17.33  \n",
      "   1    |   40    |   0.171638   |     -      |     -     |   16.27  \n",
      "   1    |   60    |   0.176990   |     -      |     -     |   16.31  \n",
      "   1    |   80    |   0.195353   |     -      |     -     |   16.33  \n",
      "   1    |   100   |   0.162904   |     -      |     -     |   16.34  \n",
      "   1    |   120   |   0.183685   |     -      |     -     |   16.37  \n",
      "   1    |   140   |   0.164266   |     -      |     -     |   16.35  \n",
      "   1    |   160   |   0.154233   |     -      |     -     |   16.33  \n",
      "   1    |   180   |   0.160069   |     -      |     -     |   16.33  \n",
      "   1    |   200   |   0.156313   |     -      |     -     |   16.35  \n",
      "   1    |   220   |   0.140911   |     -      |     -     |   16.34  \n",
      "   1    |   240   |   0.148607   |     -      |     -     |   16.35  \n",
      "   1    |   260   |   0.176226   |     -      |     -     |   16.32  \n",
      "   1    |   280   |   0.156780   |     -      |     -     |   16.31  \n",
      "   1    |   300   |   0.150919   |     -      |     -     |   16.31  \n",
      "   1    |   320   |   0.147350   |     -      |     -     |   16.27  \n",
      "   1    |   340   |   0.143975   |     -      |     -     |   16.36  \n",
      "   1    |   360   |   0.182671   |     -      |     -     |   16.35  \n",
      "   1    |   380   |   0.154394   |     -      |     -     |   16.34  \n",
      "   1    |   400   |   0.153900   |     -      |     -     |   16.34  \n",
      "   1    |   420   |   0.150135   |     -      |     -     |   16.36  \n",
      "   1    |   440   |   0.155689   |     -      |     -     |   16.39  \n",
      "   1    |   460   |   0.144827   |     -      |     -     |   16.37  \n",
      "   1    |   480   |   0.162006   |     -      |     -     |   16.36  \n",
      "   1    |   490   |   0.150906   |     -      |     -     |   7.89   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.160342   |  0.163108  |   75.91   |  418.66  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 3\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.155521   |     -      |     -     |   17.59  \n",
      "   1    |   40    |   0.147545   |     -      |     -     |   16.04  \n",
      "   1    |   60    |   0.141441   |     -      |     -     |   16.02  \n",
      "   1    |   80    |   0.138160   |     -      |     -     |   16.01  \n",
      "   1    |   100   |   0.116081   |     -      |     -     |   16.03  \n",
      "   1    |   120   |   0.145738   |     -      |     -     |   16.01  \n",
      "   1    |   140   |   0.157535   |     -      |     -     |   16.07  \n",
      "   1    |   160   |   0.149859   |     -      |     -     |   17.67  \n",
      "   1    |   180   |   0.143652   |     -      |     -     |   16.35  \n",
      "   1    |   200   |   0.141576   |     -      |     -     |   16.16  \n",
      "   1    |   220   |   0.144511   |     -      |     -     |   16.17  \n",
      "   1    |   240   |   0.130923   |     -      |     -     |   16.19  \n",
      "   1    |   260   |   0.133950   |     -      |     -     |   16.16  \n",
      "   1    |   280   |   0.146669   |     -      |     -     |   16.19  \n",
      "   1    |   300   |   0.125344   |     -      |     -     |   16.19  \n",
      "   1    |   320   |   0.137822   |     -      |     -     |   16.16  \n",
      "   1    |   340   |   0.126345   |     -      |     -     |   16.14  \n",
      "   1    |   360   |   0.143697   |     -      |     -     |   16.14  \n",
      "   1    |   380   |   0.132113   |     -      |     -     |   16.20  \n",
      "   1    |   400   |   0.135302   |     -      |     -     |   16.20  \n",
      "   1    |   420   |   0.137255   |     -      |     -     |   16.16  \n",
      "   1    |   440   |   0.114496   |     -      |     -     |   16.09  \n",
      "   1    |   460   |   0.150481   |     -      |     -     |   16.31  \n",
      "   1    |   480   |   0.132982   |     -      |     -     |   16.68  \n",
      "   1    |   490   |   0.128844   |     -      |     -     |   8.02   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.138542   |  0.163108  |   75.91   |  417.18  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Fold 6\n",
      "Epoch 1\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.344163   |     -      |     -     |   17.02  \n",
      "   1    |   40    |   0.249479   |     -      |     -     |   16.32  \n",
      "   1    |   60    |   0.234490   |     -      |     -     |   16.42  \n",
      "   1    |   80    |   0.220915   |     -      |     -     |   16.67  \n",
      "   1    |   100   |   0.195792   |     -      |     -     |   16.72  \n",
      "   1    |   120   |   0.215777   |     -      |     -     |   16.71  \n",
      "   1    |   140   |   0.203942   |     -      |     -     |   16.74  \n",
      "   1    |   160   |   0.220419   |     -      |     -     |   16.71  \n",
      "   1    |   180   |   0.196812   |     -      |     -     |   16.75  \n",
      "   1    |   200   |   0.192428   |     -      |     -     |   16.71  \n",
      "   1    |   220   |   0.194484   |     -      |     -     |   16.71  \n",
      "   1    |   240   |   0.198808   |     -      |     -     |   16.74  \n",
      "   1    |   260   |   0.196192   |     -      |     -     |   16.42  \n",
      "   1    |   280   |   0.189286   |     -      |     -     |   15.85  \n",
      "   1    |   300   |   0.179717   |     -      |     -     |   15.84  \n",
      "   1    |   320   |   0.212690   |     -      |     -     |   15.84  \n",
      "   1    |   340   |   0.183414   |     -      |     -     |   16.84  \n",
      "   1    |   360   |   0.187811   |     -      |     -     |   17.68  \n",
      "   1    |   380   |   0.194267   |     -      |     -     |   16.72  \n",
      "   1    |   400   |   0.204876   |     -      |     -     |   16.47  \n",
      "   1    |   420   |   0.215238   |     -      |     -     |   15.82  \n",
      "   1    |   440   |   0.200697   |     -      |     -     |   15.72  \n",
      "   1    |   460   |   0.191112   |     -      |     -     |   15.76  \n",
      "   1    |   480   |   0.197568   |     -      |     -     |   15.73  \n",
      "   1    |   490   |   0.155288   |     -      |     -     |   7.48   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.208360   |  0.167883  |   72.73   |  419.96  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 2\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.158471   |     -      |     -     |   17.91  \n",
      "   1    |   40    |   0.158199   |     -      |     -     |   16.52  \n",
      "   1    |   60    |   0.157926   |     -      |     -     |   16.55  \n",
      "   1    |   80    |   0.168198   |     -      |     -     |   15.76  \n",
      "   1    |   100   |   0.170561   |     -      |     -     |   15.79  \n",
      "   1    |   120   |   0.156635   |     -      |     -     |   15.75  \n",
      "   1    |   140   |   0.198276   |     -      |     -     |   16.26  \n",
      "   1    |   160   |   0.173395   |     -      |     -     |   16.30  \n",
      "   1    |   180   |   0.165714   |     -      |     -     |   16.30  \n",
      "   1    |   200   |   0.155343   |     -      |     -     |   16.33  \n",
      "   1    |   220   |   0.170004   |     -      |     -     |   16.28  \n",
      "   1    |   240   |   0.152567   |     -      |     -     |   16.31  \n",
      "   1    |   260   |   0.168365   |     -      |     -     |   16.31  \n",
      "   1    |   280   |   0.145617   |     -      |     -     |   16.21  \n",
      "   1    |   300   |   0.159988   |     -      |     -     |   16.22  \n",
      "   1    |   320   |   0.159676   |     -      |     -     |   16.24  \n",
      "   1    |   340   |   0.166512   |     -      |     -     |   16.29  \n",
      "   1    |   360   |   0.161423   |     -      |     -     |   16.34  \n",
      "   1    |   380   |   0.160146   |     -      |     -     |   16.26  \n",
      "   1    |   400   |   0.149246   |     -      |     -     |   16.29  \n",
      "   1    |   420   |   0.158920   |     -      |     -     |   16.29  \n",
      "   1    |   440   |   0.162968   |     -      |     -     |   16.33  \n",
      "   1    |   460   |   0.154025   |     -      |     -     |   16.26  \n",
      "   1    |   480   |   0.143916   |     -      |     -     |   16.27  \n",
      "   1    |   490   |   0.164677   |     -      |     -     |   7.83   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.161562   |  0.159033  |   74.89   |  416.67  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 3\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.145816   |     -      |     -     |   17.65  \n",
      "   1    |   40    |   0.158915   |     -      |     -     |   15.97  \n",
      "   1    |   60    |   0.162380   |     -      |     -     |   15.89  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a directory to save the results\n",
    "rootdir = \"LRADAMWDROPOUT_alBERT_fold_results\"\n",
    "if not os.path.exists(\"%s\" % rootdir):\n",
    "    os.mkdir(rootdir)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "batch_size = 16# Loop over each fold\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"Fold {fold}\")\n",
    "    fold_metrics = []\n",
    "    probsandlab = [['Probability', 'Label']]\n",
    "    # Create a directory for the current fold\n",
    "    fold_dir = f\"%s/fold_{fold}\" % rootdir\n",
    "    if not os.path.exists(fold_dir):\n",
    "        os.mkdir(fold_dir)\n",
    "\n",
    "    # Create the DataLoader for our training set\n",
    "    train_data = TensorDataset(X_inputs[train_idx], X_masks[train_idx], y[train_idx])\n",
    "    train_sampler = RandomSampler(train_data) # No adjust weight\n",
    "    # train_sampler = train_subsampler # Adjusted weight\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create the DataLoader for our validation set\n",
    "    val_data = TensorDataset(X_inputs[val_idx], X_masks[val_idx], y[val_idx])\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create a directory to save the fold's epoch results\n",
    "    epoch_results_dir = f\"{fold_dir}/epoch_results\"\n",
    "    if not os.path.exists(epoch_results_dir):\n",
    "        os.mkdir(epoch_results_dir)\n",
    "\n",
    "    set_seed(42)    # Set seed for reproducibility\n",
    "    bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch in range(1, 4):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        train(bert_classifier, train_dataloader, val_dataloader, epochs=1, evaluation=True)\n",
    "\n",
    "        # Save the model\n",
    "        model_path = f\"{epoch_results_dir}/model_epoch_{epoch}.pt\"\n",
    "        torch.save(bert_classifier.state_dict(), model_path)\n",
    "\n",
    "        # Compute predicted probabilities on the validation set\n",
    "        val_probs = bert_predict(bert_classifier, val_dataloader)\n",
    "        val_preds = (val_probs > 0.5).astype(int)\n",
    "        val_labels = y[val_idx].detach().numpy()\n",
    "\n",
    "        # Convert val_probs and val_labels to lists\n",
    "        val_probs_list = val_probs.tolist()\n",
    "        val_labels_list = val_labels.tolist()\n",
    "\n",
    "        # Prepare the data for writing to CSV\n",
    "        probsandlab.extend(list(zip(val_probs_list, val_labels_list)))\n",
    "\n",
    "        # Create a DataFrame for the fold's metrics\n",
    "        fold_metrics.append({\n",
    "            'epoch': epoch,\n",
    "            'predicted_probs': val_probs,\n",
    "            'predicted_label': val_preds,\n",
    "            'true_label': val_labels,\n",
    "            'accuracy': accuracy_score(val_labels, val_preds),\n",
    "            'precision': precision_score(val_labels, val_preds),\n",
    "            'recall': recall_score(val_labels, val_preds),\n",
    "            'f1': f1_score(val_labels, val_preds),\n",
    "        })\n",
    "\n",
    "        # Save the classification report to a text file\n",
    "        report_path = f\"{epoch_results_dir}/classification_report_epoch_{epoch}.txt\"\n",
    "        with open(report_path, 'w') as report_file:\n",
    "            report_file.write(classification_report(val_labels, val_preds,digits=6))\n",
    "\n",
    "    metricss_path = f\"{fold_dir}/probsandlab_{fold}_bert.csv\"\n",
    "    probsandlab_df = pd.DataFrame(probsandlab[1:], columns=probsandlab[0])\n",
    "    probsandlab_df.to_csv(metricss_path, index=False)\n",
    "\n",
    "# Create a DataFrame for the fold's metrics\n",
    "    fold_df = pd.DataFrame(fold_metrics)\n",
    "\n",
    "    # Save the fold's metrics to a CSV file\n",
    "    metrics_path = f\"{fold_dir}/fold_{fold}_bert.csv\"\n",
    "    fold_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "    # Plot the validation accuracy over the number of epochs\n",
    "    plt.plot(fold_df['epoch'], fold_df['accuracy'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title(f'Fold {fold} - Validation Accuracy')\n",
    "    plt.savefig(f\"{fold_dir}/fold_{fold}_accuracy_plot.jpg\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
