{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Essential Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "transformers.logging.set_verbosity_error()\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up GPU for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "Device name: TITAN RTX\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# if torch.backends.mps.is_available():\n",
    "#     device = torch.device(\"mps\")\n",
    "#     print('mps supported')\n",
    "# else:\n",
    "#     print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#read file\n",
    "df = pd.read_csv('train.csv')\n",
    "df1 = pd.read_csv('dev.csv')\n",
    "\n",
    "df1 = df1.rename(columns={'labels': 'label'})\n",
    "\n",
    "df = pd.concat([df, df1], ignore_index=True)\n",
    "\n",
    "#select only text, tweet ids, sentiment label and sentiment agree columns\n",
    "df = df[['text','label']]\n",
    "\n",
    "X = df[['text']]\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8722, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "LEARNING_RATE = 2e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "\n",
    "    text = text\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b167776de07456d821ebc79e3a4c76d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4a7c5d68db94a1baf009c0950052994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebc986db87e5495cad1dbdb1b9b8d7ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer\n",
    "# Load the BERT tokenizer\n",
    "pretrained_bertmodel = 'distilbert-base-uncased'  # Specify the ALBERT model you want to use\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(pretrained_bertmodel)\n",
    "\n",
    "# Create a function to tokenize a set of texts\n",
    "def preprocessing_for_bert(df):\n",
    "    \"\"\"Perform required preprocessing steps for pretrained BERT.\n",
    "    @param    df (pd.DataFrame): DataFrame containing text1 and text2 columns to be processed.\n",
    "    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n",
    "    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n",
    "                  tokens should be attended to by the model.\n",
    "    \"\"\"\n",
    "    # Create empty lists to store outputs\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    # For every row in the dataframe...\n",
    "    for index, row in df.iterrows():\n",
    "        # `encode_plus` will:\n",
    "        #    (1) Tokenize the sentences\n",
    "        #    (2) Add the `[CLS]` and `[SEP]` tokens to the start and end\n",
    "        #    (3) Truncate/Pad sentences to max length\n",
    "        #    (4) Map tokens to their IDs\n",
    "        #    (5) Create attention masks\n",
    "        #    (6) Return a dictionary of outputs\n",
    "        encoded_sent = tokenizer.encode_plus(\n",
    "            text=text_preprocessing(row['text']),\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LEN,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True\n",
    "            )\n",
    "\n",
    "        # Add the outputs to the lists\n",
    "        input_ids.append(encoded_sent.get('input_ids'))\n",
    "        attention_masks.append(encoded_sent.get('attention_mask'))\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.tensor(input_ids)\n",
    "    attention_masks = torch.tensor(attention_masks)\n",
    "\n",
    "    return input_ids, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eugene/.local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_inputs, X_masks = preprocessing_for_bert(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "y=torch.tensor(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create BertClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import DistilBertModel\n",
    "\n",
    "# Create the BertClassfier class\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Bert Model for Binary Classification Tasks.\n",
    "    \"\"\"\n",
    "    def __init__(self, freeze_bert=False):\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        D_in, H, D_out = 768, 50, 1  # Change D_out to 1 for binary classification\n",
    "\n",
    "        self.bert = DistilBertModel.from_pretrained(pretrained_bertmodel)\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(D_in, H),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.35),\n",
    "            nn.Linear(H, D_out)\n",
    "        )\n",
    "\n",
    "        # Freeze the BERT model\n",
    "        if freeze_bert:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state_cls = outputs[0][:, 0, :]\n",
    "        logits = self.classifier(last_hidden_state_cls)\n",
    "        return logits.squeeze()  # Squeeze the output to remove the extra dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer & Learning Rate Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    \"\"\"Initialize the Bert Classifier, the optimizer, and the learning rate scheduler.\"\"\"\n",
    "    # Instantiate Bert Classifier\n",
    "    bert_classifier = BertClassifier(freeze_bert=False)\n",
    "\n",
    "    # Tell PyTorch to run the model on GPU\n",
    "    bert_classifier.to(device)\n",
    "\n",
    "    # Create the optimizer\n",
    "    optimizer = torch.optim.AdamW(bert_classifier.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    # Total number of training steps\n",
    "    total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "    # Set up the learning rate scheduler\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                                num_warmup_steps=0,  # Default value\n",
    "                                                num_training_steps=total_steps)\n",
    "    return bert_classifier, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# Specify loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "threshold = 0.5\n",
    "# loss_fn = nn.CrossEntropyLoss() # No adjust weight\n",
    "# loss_fn = nn.CrossEntropyLoss(weight = torch.tensor(class_weights, dtype=torch.float)) # Adjust weight\n",
    "\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "\n",
    "def train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n",
    "    \"\"\"Train the BertClassifier model.\n",
    "    \"\"\"\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n",
    "        print(\"-\"*70)\n",
    "\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "\n",
    "        # Put the model into the training mode\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch_counts +=1\n",
    "            # Load batch to GPU\n",
    "            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "            # Convert true labels to Float data type\n",
    "            b_labels = b_labels.float()\n",
    "\n",
    "            # Zero out any previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Compute logits\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = loss_fn(logits.view(-1), b_labels.view(-1))\n",
    "\n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and the learning rate\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Print the loss values and time elapsed for every 20 batches\n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "\n",
    "                # Print training results\n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n",
    "\n",
    "                # Reset batch tracking variables\n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "\n",
    "        # Calculate the average loss over the entire training data\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "        print(\"-\"*70)\n",
    "        # =======================================\n",
    "        #               Evaluation\n",
    "        # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "\n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n",
    "            print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    val_accuracy = []\n",
    "    val_loss = []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_fn(logits.squeeze(), b_labels.float())\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        preds = (logits > threshold).long().flatten()\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        val_accuracy.append(accuracy)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT PREDICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def bert_predict(model, test_dataloader, threshold=0.5):\n",
    "    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n",
    "    on the test set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers care disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    all_logits = []\n",
    "\n",
    "    # For each batch in our test set...\n",
    "    for batch in test_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Apply sigmoid to convert logits to probabilities\n",
    "        probs = logits.cpu()\n",
    "#         probs = torch.sigmoid(logits).cpu().numpy()\n",
    "        all_logits.append(probs)\n",
    "\n",
    "    # Concatenate probabilities from each batch\n",
    "    all_probs = np.concatenate(all_logits, axis=0)\n",
    "\n",
    "    return all_probs\n",
    "\n",
    "# # Compute predicted probabilities on the test set\n",
    "# probs = bert_predict(bert_classifier, val_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KFOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d978630b7b446ae8454d86ef6176f99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.278829   |     -      |     -     |   5.66   \n",
      "   1    |   40    |   0.235246   |     -      |     -     |   5.21   \n",
      "   1    |   60    |   0.221111   |     -      |     -     |   5.21   \n",
      "   1    |   80    |   0.215079   |     -      |     -     |   5.24   \n",
      "   1    |   100   |   0.201509   |     -      |     -     |   5.21   \n",
      "   1    |   120   |   0.208999   |     -      |     -     |   5.22   \n",
      "   1    |   140   |   0.222328   |     -      |     -     |   5.24   \n",
      "   1    |   160   |   0.206645   |     -      |     -     |   5.27   \n",
      "   1    |   180   |   0.187001   |     -      |     -     |   5.28   \n",
      "   1    |   200   |   0.181950   |     -      |     -     |   5.28   \n",
      "   1    |   220   |   0.227307   |     -      |     -     |   5.28   \n",
      "   1    |   240   |   0.217610   |     -      |     -     |   5.30   \n",
      "   1    |   260   |   0.189447   |     -      |     -     |   5.33   \n",
      "   1    |   280   |   0.197324   |     -      |     -     |   5.32   \n",
      "   1    |   300   |   0.198582   |     -      |     -     |   5.34   \n",
      "   1    |   320   |   0.198339   |     -      |     -     |   5.34   \n",
      "   1    |   340   |   0.201812   |     -      |     -     |   5.35   \n",
      "   1    |   360   |   0.203393   |     -      |     -     |   5.35   \n",
      "   1    |   380   |   0.185884   |     -      |     -     |   5.36   \n",
      "   1    |   400   |   0.166806   |     -      |     -     |   5.40   \n",
      "   1    |   420   |   0.210715   |     -      |     -     |   5.43   \n",
      "   1    |   440   |   0.194065   |     -      |     -     |   5.43   \n",
      "   1    |   460   |   0.176397   |     -      |     -     |   5.45   \n",
      "   1    |   480   |   0.201132   |     -      |     -     |   5.47   \n",
      "   1    |   490   |   0.152918   |     -      |     -     |   2.62   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.204396   |  0.175680  |   74.57   |  135.44  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 2\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.181705   |     -      |     -     |   5.91   \n",
      "   1    |   40    |   0.177242   |     -      |     -     |   5.75   \n",
      "   1    |   60    |   0.169676   |     -      |     -     |   5.81   \n",
      "   1    |   80    |   0.152822   |     -      |     -     |   5.88   \n",
      "   1    |   100   |   0.150140   |     -      |     -     |   6.01   \n",
      "   1    |   120   |   0.140128   |     -      |     -     |   6.07   \n",
      "   1    |   140   |   0.172254   |     -      |     -     |   6.08   \n",
      "   1    |   160   |   0.165306   |     -      |     -     |   6.15   \n",
      "   1    |   180   |   0.155529   |     -      |     -     |   6.18   \n",
      "   1    |   200   |   0.154874   |     -      |     -     |   6.17   \n",
      "   1    |   220   |   0.172757   |     -      |     -     |   6.16   \n",
      "   1    |   240   |   0.154318   |     -      |     -     |   6.18   \n",
      "   1    |   260   |   0.166964   |     -      |     -     |   6.19   \n",
      "   1    |   280   |   0.142619   |     -      |     -     |   6.26   \n",
      "   1    |   300   |   0.175429   |     -      |     -     |   6.51   \n",
      "   1    |   320   |   0.151894   |     -      |     -     |   6.47   \n",
      "   1    |   340   |   0.141774   |     -      |     -     |   6.39   \n",
      "   1    |   360   |   0.145086   |     -      |     -     |   6.71   \n",
      "   1    |   380   |   0.173223   |     -      |     -     |   6.65   \n",
      "   1    |   400   |   0.181715   |     -      |     -     |   6.65   \n",
      "   1    |   420   |   0.145780   |     -      |     -     |   6.65   \n",
      "   1    |   440   |   0.129376   |     -      |     -     |   6.54   \n",
      "   1    |   460   |   0.130159   |     -      |     -     |   6.65   \n",
      "   1    |   480   |   0.159275   |     -      |     -     |   6.93   \n",
      "   1    |   490   |   0.178396   |     -      |     -     |   3.29   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.158384   |  0.181185  |   74.91   |  160.76  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 3\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.132760   |     -      |     -     |   7.41   \n",
      "   1    |   40    |   0.118812   |     -      |     -     |   7.21   \n",
      "   1    |   60    |   0.134983   |     -      |     -     |   7.13   \n",
      "   1    |   80    |   0.107359   |     -      |     -     |   6.94   \n",
      "   1    |   100   |   0.125063   |     -      |     -     |   7.36   \n",
      "   1    |   120   |   0.131787   |     -      |     -     |   7.14   \n",
      "   1    |   140   |   0.147524   |     -      |     -     |   7.02   \n",
      "   1    |   160   |   0.121088   |     -      |     -     |   7.12   \n",
      "   1    |   180   |   0.103720   |     -      |     -     |   7.38   \n",
      "   1    |   200   |   0.135813   |     -      |     -     |   7.34   \n",
      "   1    |   220   |   0.124182   |     -      |     -     |   7.25   \n",
      "   1    |   240   |   0.119844   |     -      |     -     |   7.14   \n",
      "   1    |   260   |   0.113427   |     -      |     -     |   7.08   \n",
      "   1    |   280   |   0.116032   |     -      |     -     |   7.25   \n",
      "   1    |   300   |   0.118421   |     -      |     -     |   7.57   \n",
      "   1    |   320   |   0.108592   |     -      |     -     |   7.59   \n",
      "   1    |   340   |   0.124058   |     -      |     -     |   7.47   \n",
      "   1    |   360   |   0.123581   |     -      |     -     |   7.53   \n",
      "   1    |   380   |   0.105173   |     -      |     -     |   7.96   \n",
      "   1    |   400   |   0.103116   |     -      |     -     |   7.55   \n",
      "   1    |   420   |   0.106457   |     -      |     -     |   7.31   \n",
      "   1    |   440   |   0.123867   |     -      |     -     |   7.27   \n",
      "   1    |   460   |   0.126938   |     -      |     -     |   7.28   \n",
      "   1    |   480   |   0.115777   |     -      |     -     |   8.23   \n",
      "   1    |   490   |   0.095445   |     -      |     -     |   3.94   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.119867   |  0.197679  |   72.41   |  188.97  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 4\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.086970   |     -      |     -     |   7.79   \n",
      "   1    |   40    |   0.083849   |     -      |     -     |   8.10   \n",
      "   1    |   60    |   0.088905   |     -      |     -     |   7.74   \n",
      "   1    |   80    |   0.071893   |     -      |     -     |   7.74   \n",
      "   1    |   100   |   0.083230   |     -      |     -     |   7.64   \n",
      "   1    |   120   |   0.109648   |     -      |     -     |   7.67   \n",
      "   1    |   140   |   0.088010   |     -      |     -     |   7.80   \n",
      "   1    |   160   |   0.071283   |     -      |     -     |   7.85   \n",
      "   1    |   180   |   0.082448   |     -      |     -     |   7.87   \n",
      "   1    |   200   |   0.090486   |     -      |     -     |   7.88   \n",
      "   1    |   220   |   0.081110   |     -      |     -     |   7.91   \n",
      "   1    |   240   |   0.070503   |     -      |     -     |   7.89   \n",
      "   1    |   260   |   0.092349   |     -      |     -     |   7.90   \n",
      "   1    |   280   |   0.102876   |     -      |     -     |   7.90   \n",
      "   1    |   300   |   0.062335   |     -      |     -     |   7.90   \n",
      "   1    |   320   |   0.068576   |     -      |     -     |   7.88   \n",
      "   1    |   340   |   0.108303   |     -      |     -     |   7.25   \n",
      "   1    |   360   |   0.087086   |     -      |     -     |   9.65   \n",
      "   1    |   380   |   0.091103   |     -      |     -     |   8.38   \n",
      "   1    |   400   |   0.082363   |     -      |     -     |   8.23   \n",
      "   1    |   420   |   0.090582   |     -      |     -     |   7.90   \n",
      "   1    |   440   |   0.089815   |     -      |     -     |   7.90   \n",
      "   1    |   460   |   0.092712   |     -      |     -     |   7.89   \n",
      "   1    |   480   |   0.095500   |     -      |     -     |   7.87   \n",
      "   1    |   490   |   0.067820   |     -      |     -     |   3.77   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.085955   |  0.230652  |   71.62   |  202.25  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 5\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.060722   |     -      |     -     |   10.28  \n",
      "   1    |   40    |   0.047685   |     -      |     -     |   8.22   \n",
      "   1    |   60    |   0.050947   |     -      |     -     |   7.97   \n",
      "   1    |   80    |   0.069249   |     -      |     -     |   7.99   \n",
      "   1    |   100   |   0.063914   |     -      |     -     |   7.98   \n",
      "   1    |   120   |   0.059677   |     -      |     -     |   8.01   \n",
      "   1    |   140   |   0.064998   |     -      |     -     |   7.70   \n",
      "   1    |   160   |   0.062840   |     -      |     -     |   7.64   \n",
      "   1    |   180   |   0.034340   |     -      |     -     |   8.02   \n",
      "   1    |   200   |   0.050336   |     -      |     -     |   8.24   \n",
      "   1    |   220   |   0.060703   |     -      |     -     |   8.21   \n",
      "   1    |   240   |   0.055577   |     -      |     -     |   8.20   \n",
      "   1    |   260   |   0.078918   |     -      |     -     |   7.66   \n",
      "   1    |   280   |   0.059826   |     -      |     -     |   7.65   \n",
      "   1    |   300   |   0.053271   |     -      |     -     |   8.06   \n",
      "   1    |   320   |   0.057828   |     -      |     -     |   7.83   \n",
      "   1    |   340   |   0.062105   |     -      |     -     |   7.69   \n",
      "   1    |   360   |   0.055777   |     -      |     -     |   7.45   \n",
      "   1    |   380   |   0.091201   |     -      |     -     |   7.98   \n",
      "   1    |   400   |   0.046153   |     -      |     -     |   8.11   \n",
      "   1    |   420   |   0.067474   |     -      |     -     |   8.12   \n",
      "   1    |   440   |   0.049522   |     -      |     -     |   7.88   \n",
      "   1    |   460   |   0.050268   |     -      |     -     |   7.73   \n",
      "   1    |   480   |   0.057922   |     -      |     -     |   7.63   \n",
      "   1    |   490   |   0.054081   |     -      |     -     |   3.67   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.058710   |  0.233110  |   71.62   |  203.63  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Fold 1\n",
      "Epoch 1\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.278694   |     -      |     -     |   8.02   \n",
      "   1    |   40    |   0.233713   |     -      |     -     |   7.93   \n",
      "   1    |   60    |   0.214894   |     -      |     -     |   7.92   \n",
      "   1    |   80    |   0.224408   |     -      |     -     |   7.92   \n",
      "   1    |   100   |   0.210655   |     -      |     -     |   7.90   \n",
      "   1    |   120   |   0.209550   |     -      |     -     |   7.92   \n",
      "   1    |   140   |   0.190142   |     -      |     -     |   7.50   \n",
      "   1    |   160   |   0.194682   |     -      |     -     |   7.75   \n",
      "   1    |   180   |   0.208087   |     -      |     -     |   8.23   \n",
      "   1    |   200   |   0.195004   |     -      |     -     |   8.20   \n",
      "   1    |   220   |   0.185245   |     -      |     -     |   8.20   \n",
      "   1    |   240   |   0.185402   |     -      |     -     |   8.18   \n",
      "   1    |   260   |   0.209777   |     -      |     -     |   8.20   \n",
      "   1    |   280   |   0.209254   |     -      |     -     |   8.15   \n",
      "   1    |   300   |   0.191253   |     -      |     -     |   8.07   \n",
      "   1    |   320   |   0.185179   |     -      |     -     |   7.45   \n",
      "   1    |   340   |   0.195483   |     -      |     -     |   7.50   \n",
      "   1    |   360   |   0.185122   |     -      |     -     |   7.50   \n",
      "   1    |   380   |   0.186361   |     -      |     -     |   8.10   \n",
      "   1    |   400   |   0.175376   |     -      |     -     |   7.89   \n",
      "   1    |   420   |   0.176009   |     -      |     -     |   7.57   \n",
      "   1    |   440   |   0.188005   |     -      |     -     |   7.47   \n",
      "   1    |   460   |   0.210026   |     -      |     -     |   7.61   \n",
      "   1    |   480   |   0.188636   |     -      |     -     |   7.87   \n",
      "   1    |   490   |   0.165534   |     -      |     -     |   3.77   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.200719   |  0.171326  |   75.27   |  200.72  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 2\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.155042   |     -      |     -     |   8.93   \n",
      "   1    |   40    |   0.176829   |     -      |     -     |   8.96   \n",
      "   1    |   60    |   0.159137   |     -      |     -     |   8.02   \n",
      "   1    |   80    |   0.184227   |     -      |     -     |   7.99   \n",
      "   1    |   100   |   0.175650   |     -      |     -     |   8.02   \n",
      "   1    |   120   |   0.151224   |     -      |     -     |   8.01   \n",
      "   1    |   140   |   0.154538   |     -      |     -     |   7.80   \n",
      "   1    |   160   |   0.167697   |     -      |     -     |   7.65   \n",
      "   1    |   180   |   0.167783   |     -      |     -     |   7.71   \n",
      "   1    |   200   |   0.147052   |     -      |     -     |   7.81   \n",
      "   1    |   220   |   0.172223   |     -      |     -     |   7.86   \n",
      "   1    |   240   |   0.182690   |     -      |     -     |   7.79   \n",
      "   1    |   260   |   0.169923   |     -      |     -     |   7.76   \n",
      "   1    |   280   |   0.152927   |     -      |     -     |   7.83   \n",
      "   1    |   300   |   0.139818   |     -      |     -     |   7.84   \n",
      "   1    |   320   |   0.152150   |     -      |     -     |   7.86   \n",
      "   1    |   340   |   0.170531   |     -      |     -     |   7.84   \n",
      "   1    |   360   |   0.166278   |     -      |     -     |   7.85   \n",
      "   1    |   380   |   0.158929   |     -      |     -     |   7.65   \n",
      "   1    |   400   |   0.171931   |     -      |     -     |   8.01   \n",
      "   1    |   420   |   0.150781   |     -      |     -     |   8.05   \n",
      "   1    |   440   |   0.150645   |     -      |     -     |   8.07   \n",
      "   1    |   460   |   0.142829   |     -      |     -     |   8.03   \n",
      "   1    |   480   |   0.160671   |     -      |     -     |   8.07   \n",
      "   1    |   490   |   0.128803   |     -      |     -     |   3.86   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.161045   |  0.171719  |   75.47   |  203.55  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 3\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.113648   |     -      |     -     |   9.37   \n",
      "   1    |   40    |   0.094249   |     -      |     -     |   8.11   \n",
      "   1    |   60    |   0.154489   |     -      |     -     |   8.08   \n",
      "   1    |   80    |   0.117059   |     -      |     -     |   8.08   \n",
      "   1    |   100   |   0.120929   |     -      |     -     |   7.94   \n",
      "   1    |   120   |   0.124038   |     -      |     -     |   7.70   \n",
      "   1    |   140   |   0.131416   |     -      |     -     |   7.71   \n",
      "   1    |   160   |   0.124734   |     -      |     -     |   7.66   \n",
      "   1    |   180   |   0.136196   |     -      |     -     |   7.70   \n",
      "   1    |   200   |   0.143095   |     -      |     -     |   8.33   \n",
      "   1    |   220   |   0.122521   |     -      |     -     |   8.50   \n",
      "   1    |   240   |   0.127549   |     -      |     -     |   7.79   \n",
      "   1    |   260   |   0.119354   |     -      |     -     |   7.77   \n",
      "   1    |   280   |   0.112465   |     -      |     -     |   7.77   \n",
      "   1    |   300   |   0.138453   |     -      |     -     |   7.80   \n",
      "   1    |   320   |   0.115606   |     -      |     -     |   7.78   \n",
      "   1    |   340   |   0.137913   |     -      |     -     |   7.79   \n",
      "   1    |   360   |   0.119789   |     -      |     -     |   7.78   \n",
      "   1    |   380   |   0.099237   |     -      |     -     |   7.78   \n",
      "   1    |   400   |   0.118688   |     -      |     -     |   7.81   \n",
      "   1    |   420   |   0.146684   |     -      |     -     |   7.61   \n",
      "   1    |   440   |   0.124825   |     -      |     -     |   8.62   \n",
      "   1    |   460   |   0.128880   |     -      |     -     |   8.72   \n",
      "   1    |   480   |   0.126693   |     -      |     -     |   7.92   \n",
      "   1    |   490   |   0.181188   |     -      |     -     |   3.69   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.126061   |  0.178502  |   74.73   |  203.38  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 4\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.091089   |     -      |     -     |   9.46   \n",
      "   1    |   40    |   0.095858   |     -      |     -     |   8.20   \n",
      "   1    |   60    |   0.101387   |     -      |     -     |   7.71   \n",
      "   1    |   80    |   0.076061   |     -      |     -     |   7.87   \n",
      "   1    |   100   |   0.075866   |     -      |     -     |   8.89   \n",
      "   1    |   120   |   0.066098   |     -      |     -     |   8.11   \n",
      "   1    |   140   |   0.107329   |     -      |     -     |   8.13   \n",
      "   1    |   160   |   0.101976   |     -      |     -     |   7.87   \n",
      "   1    |   180   |   0.086479   |     -      |     -     |   7.76   \n",
      "   1    |   200   |   0.073393   |     -      |     -     |   7.73   \n",
      "   1    |   220   |   0.075444   |     -      |     -     |   7.75   \n",
      "   1    |   240   |   0.070657   |     -      |     -     |   7.64   \n",
      "   1    |   260   |   0.082056   |     -      |     -     |   7.98   \n",
      "   1    |   280   |   0.097961   |     -      |     -     |   7.98   \n",
      "   1    |   300   |   0.077878   |     -      |     -     |   7.99   \n",
      "   1    |   320   |   0.095889   |     -      |     -     |   7.98   \n",
      "   1    |   340   |   0.075952   |     -      |     -     |   7.98   \n",
      "   1    |   360   |   0.088927   |     -      |     -     |   7.99   \n",
      "   1    |   380   |   0.078123   |     -      |     -     |   7.96   \n",
      "   1    |   400   |   0.089399   |     -      |     -     |   7.95   \n",
      "   1    |   420   |   0.090220   |     -      |     -     |   7.96   \n",
      "   1    |   440   |   0.086570   |     -      |     -     |   7.98   \n",
      "   1    |   460   |   0.090195   |     -      |     -     |   7.96   \n",
      "   1    |   480   |   0.069905   |     -      |     -     |   7.94   \n",
      "   1    |   490   |   0.095093   |     -      |     -     |   3.82   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.085410   |  0.195839  |   75.30   |  204.66  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 5\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.041593   |     -      |     -     |   8.61   \n",
      "   1    |   40    |   0.073870   |     -      |     -     |   8.32   \n",
      "   1    |   60    |   0.061817   |     -      |     -     |   8.35   \n",
      "   1    |   80    |   0.064484   |     -      |     -     |   8.21   \n",
      "   1    |   100   |   0.070013   |     -      |     -     |   7.67   \n",
      "   1    |   120   |   0.067669   |     -      |     -     |   7.49   \n",
      "   1    |   140   |   0.059237   |     -      |     -     |   8.65   \n",
      "   1    |   160   |   0.052397   |     -      |     -     |   7.85   \n",
      "   1    |   180   |   0.043162   |     -      |     -     |   7.80   \n",
      "   1    |   200   |   0.062501   |     -      |     -     |   7.81   \n",
      "   1    |   220   |   0.057684   |     -      |     -     |   7.81   \n",
      "   1    |   240   |   0.058285   |     -      |     -     |   7.80   \n",
      "   1    |   260   |   0.051713   |     -      |     -     |   7.80   \n",
      "   1    |   280   |   0.060449   |     -      |     -     |   7.80   \n",
      "   1    |   300   |   0.058552   |     -      |     -     |   7.80   \n",
      "   1    |   320   |   0.060316   |     -      |     -     |   7.95   \n",
      "   1    |   340   |   0.068905   |     -      |     -     |   8.36   \n",
      "   1    |   360   |   0.062933   |     -      |     -     |   8.32   \n",
      "   1    |   380   |   0.066915   |     -      |     -     |   8.29   \n",
      "   1    |   400   |   0.061134   |     -      |     -     |   7.89   \n",
      "   1    |   420   |   0.060575   |     -      |     -     |   7.89   \n",
      "   1    |   440   |   0.080263   |     -      |     -     |   7.89   \n",
      "   1    |   460   |   0.060235   |     -      |     -     |   7.92   \n",
      "   1    |   480   |   0.064473   |     -      |     -     |   7.91   \n",
      "   1    |   490   |   0.064732   |     -      |     -     |   3.79   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.061247   |  0.211273  |   75.01   |  203.93  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Fold 2\n",
      "Epoch 1\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.287220   |     -      |     -     |   8.33   \n",
      "   1    |   40    |   0.229998   |     -      |     -     |   8.22   \n",
      "   1    |   60    |   0.220872   |     -      |     -     |   7.87   \n",
      "   1    |   80    |   0.222541   |     -      |     -     |   7.56   \n",
      "   1    |   100   |   0.214368   |     -      |     -     |   7.96   \n",
      "   1    |   120   |   0.197951   |     -      |     -     |   8.07   \n",
      "   1    |   140   |   0.195994   |     -      |     -     |   8.13   \n",
      "   1    |   160   |   0.206297   |     -      |     -     |   8.06   \n",
      "   1    |   180   |   0.183556   |     -      |     -     |   8.05   \n",
      "   1    |   200   |   0.185080   |     -      |     -     |   8.05   \n",
      "   1    |   220   |   0.179307   |     -      |     -     |   8.06   \n",
      "   1    |   240   |   0.179405   |     -      |     -     |   7.93   \n",
      "   1    |   260   |   0.206210   |     -      |     -     |   7.76   \n",
      "   1    |   280   |   0.202867   |     -      |     -     |   7.38   \n",
      "   1    |   300   |   0.205217   |     -      |     -     |   9.02   \n",
      "   1    |   320   |   0.193288   |     -      |     -     |   8.03   \n",
      "   1    |   340   |   0.209335   |     -      |     -     |   7.74   \n",
      "   1    |   360   |   0.186866   |     -      |     -     |   7.69   \n",
      "   1    |   380   |   0.192924   |     -      |     -     |   7.70   \n",
      "   1    |   400   |   0.199981   |     -      |     -     |   7.68   \n",
      "   1    |   420   |   0.208339   |     -      |     -     |   7.63   \n",
      "   1    |   440   |   0.189887   |     -      |     -     |   8.36   \n",
      "   1    |   460   |   0.183924   |     -      |     -     |   8.11   \n",
      "   1    |   480   |   0.185654   |     -      |     -     |   7.93   \n",
      "   1    |   490   |   0.182550   |     -      |     -     |   3.66   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.202555   |  0.179962  |   73.75   |  202.06  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 2\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.193761   |     -      |     -     |   8.11   \n",
      "   1    |   40    |   0.158960   |     -      |     -     |   9.01   \n",
      "   1    |   60    |   0.162276   |     -      |     -     |   9.17   \n",
      "   1    |   80    |   0.166203   |     -      |     -     |   7.93   \n",
      "   1    |   100   |   0.173375   |     -      |     -     |   7.71   \n",
      "   1    |   120   |   0.138138   |     -      |     -     |   7.67   \n",
      "   1    |   140   |   0.154633   |     -      |     -     |   7.93   \n",
      "   1    |   160   |   0.176172   |     -      |     -     |   7.91   \n",
      "   1    |   180   |   0.175037   |     -      |     -     |   7.92   \n",
      "   1    |   200   |   0.163666   |     -      |     -     |   7.91   \n",
      "   1    |   220   |   0.158311   |     -      |     -     |   7.94   \n",
      "   1    |   240   |   0.191454   |     -      |     -     |   7.92   \n",
      "   1    |   260   |   0.161080   |     -      |     -     |   7.92   \n",
      "   1    |   280   |   0.168116   |     -      |     -     |   7.92   \n",
      "   1    |   300   |   0.168199   |     -      |     -     |   7.89   \n",
      "   1    |   320   |   0.156623   |     -      |     -     |   8.47   \n",
      "   1    |   340   |   0.174603   |     -      |     -     |   8.04   \n",
      "   1    |   360   |   0.145803   |     -      |     -     |   7.95   \n",
      "   1    |   380   |   0.157201   |     -      |     -     |   7.99   \n",
      "   1    |   400   |   0.193044   |     -      |     -     |   7.96   \n",
      "   1    |   420   |   0.168732   |     -      |     -     |   7.97   \n",
      "   1    |   440   |   0.155209   |     -      |     -     |   7.81   \n",
      "   1    |   460   |   0.165425   |     -      |     -     |   7.62   \n",
      "   1    |   480   |   0.145851   |     -      |     -     |   7.65   \n",
      "   1    |   490   |   0.180698   |     -      |     -     |   3.74   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.165862   |  0.156196  |   77.27   |  204.10  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 3\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.114675   |     -      |     -     |   9.37   \n",
      "   1    |   40    |   0.124166   |     -      |     -     |   7.88   \n",
      "   1    |   60    |   0.123586   |     -      |     -     |   7.89   \n",
      "   1    |   80    |   0.140069   |     -      |     -     |   7.90   \n",
      "   1    |   100   |   0.113149   |     -      |     -     |   7.90   \n",
      "   1    |   120   |   0.140867   |     -      |     -     |   7.90   \n",
      "   1    |   140   |   0.125687   |     -      |     -     |   7.87   \n",
      "   1    |   160   |   0.138422   |     -      |     -     |   7.86   \n",
      "   1    |   180   |   0.129049   |     -      |     -     |   7.85   \n",
      "   1    |   200   |   0.127000   |     -      |     -     |   7.67   \n",
      "   1    |   220   |   0.132170   |     -      |     -     |   8.08   \n",
      "   1    |   240   |   0.123248   |     -      |     -     |   8.08   \n",
      "   1    |   260   |   0.127387   |     -      |     -     |   8.09   \n",
      "   1    |   280   |   0.127952   |     -      |     -     |   8.11   \n",
      "   1    |   300   |   0.130314   |     -      |     -     |   8.03   \n",
      "   1    |   320   |   0.147626   |     -      |     -     |   8.03   \n",
      "   1    |   340   |   0.152823   |     -      |     -     |   8.10   \n",
      "   1    |   360   |   0.138387   |     -      |     -     |   7.77   \n",
      "   1    |   380   |   0.141632   |     -      |     -     |   7.71   \n",
      "   1    |   400   |   0.148071   |     -      |     -     |   7.73   \n",
      "   1    |   420   |   0.136039   |     -      |     -     |   7.73   \n",
      "   1    |   440   |   0.154776   |     -      |     -     |   7.50   \n",
      "   1    |   460   |   0.142623   |     -      |     -     |   7.40   \n",
      "   1    |   480   |   0.121973   |     -      |     -     |   8.64   \n",
      "   1    |   490   |   0.117963   |     -      |     -     |   4.18   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.133051   |  0.154211  |   78.64   |  204.50  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 4\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.095269   |     -      |     -     |   8.17   \n",
      "   1    |   40    |   0.094357   |     -      |     -     |   7.77   \n",
      "   1    |   60    |   0.091110   |     -      |     -     |   7.76   \n",
      "   1    |   80    |   0.088843   |     -      |     -     |   7.57   \n",
      "   1    |   100   |   0.120428   |     -      |     -     |   8.43   \n",
      "   1    |   120   |   0.090328   |     -      |     -     |   8.35   \n",
      "   1    |   140   |   0.104192   |     -      |     -     |   8.10   \n",
      "   1    |   160   |   0.104404   |     -      |     -     |   8.12   \n",
      "   1    |   180   |   0.094070   |     -      |     -     |   8.10   \n",
      "   1    |   200   |   0.097274   |     -      |     -     |   8.03   \n",
      "   1    |   220   |   0.111207   |     -      |     -     |   7.73   \n",
      "   1    |   240   |   0.083334   |     -      |     -     |   7.76   \n",
      "   1    |   260   |   0.125455   |     -      |     -     |   7.73   \n",
      "   1    |   280   |   0.095652   |     -      |     -     |   7.73   \n",
      "   1    |   300   |   0.108328   |     -      |     -     |   7.73   \n",
      "   1    |   320   |   0.090394   |     -      |     -     |   7.62   \n",
      "   1    |   340   |   0.115438   |     -      |     -     |   7.83   \n",
      "   1    |   360   |   0.116299   |     -      |     -     |   7.94   \n",
      "   1    |   380   |   0.103506   |     -      |     -     |   8.42   \n",
      "   1    |   400   |   0.098660   |     -      |     -     |   8.41   \n",
      "   1    |   420   |   0.082148   |     -      |     -     |   7.93   \n",
      "   1    |   440   |   0.090647   |     -      |     -     |   7.94   \n",
      "   1    |   460   |   0.090085   |     -      |     -     |   7.95   \n",
      "   1    |   480   |   0.090232   |     -      |     -     |   7.94   \n",
      "   1    |   490   |   0.101061   |     -      |     -     |   3.69   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.099265   |  0.177726  |   77.61   |  202.21  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 5\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.068164   |     -      |     -     |   8.50   \n",
      "   1    |   40    |   0.053998   |     -      |     -     |   8.31   \n",
      "   1    |   60    |   0.073126   |     -      |     -     |   8.32   \n",
      "   1    |   80    |   0.073490   |     -      |     -     |   8.27   \n",
      "   1    |   100   |   0.066313   |     -      |     -     |   8.26   \n",
      "   1    |   120   |   0.086513   |     -      |     -     |   7.87   \n",
      "   1    |   140   |   0.080125   |     -      |     -     |   7.81   \n",
      "   1    |   160   |   0.059246   |     -      |     -     |   7.77   \n",
      "   1    |   180   |   0.079278   |     -      |     -     |   7.79   \n",
      "   1    |   200   |   0.069074   |     -      |     -     |   7.88   \n",
      "   1    |   220   |   0.085327   |     -      |     -     |   7.90   \n",
      "   1    |   240   |   0.062966   |     -      |     -     |   7.90   \n",
      "   1    |   260   |   0.080031   |     -      |     -     |   7.88   \n",
      "   1    |   280   |   0.072879   |     -      |     -     |   7.86   \n",
      "   1    |   300   |   0.064059   |     -      |     -     |   7.88   \n",
      "   1    |   320   |   0.064447   |     -      |     -     |   7.89   \n",
      "   1    |   340   |   0.056690   |     -      |     -     |   7.88   \n",
      "   1    |   360   |   0.081504   |     -      |     -     |   7.88   \n",
      "   1    |   380   |   0.078201   |     -      |     -     |   7.88   \n",
      "   1    |   400   |   0.064545   |     -      |     -     |   7.85   \n",
      "   1    |   420   |   0.094733   |     -      |     -     |   7.88   \n",
      "   1    |   440   |   0.053132   |     -      |     -     |   7.87   \n",
      "   1    |   460   |   0.064670   |     -      |     -     |   7.88   \n",
      "   1    |   480   |   0.064597   |     -      |     -     |   7.98   \n",
      "   1    |   490   |   0.064276   |     -      |     -     |   4.13   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.070577   |  0.194487  |   76.25   |  203.82  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Fold 3\n",
      "Epoch 1\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.292668   |     -      |     -     |   7.80   \n",
      "   1    |   40    |   0.236786   |     -      |     -     |   9.20   \n",
      "   1    |   60    |   0.227321   |     -      |     -     |   8.34   \n",
      "   1    |   80    |   0.202666   |     -      |     -     |   7.70   \n",
      "   1    |   100   |   0.190962   |     -      |     -     |   7.70   \n",
      "   1    |   120   |   0.202718   |     -      |     -     |   7.71   \n",
      "   1    |   140   |   0.216656   |     -      |     -     |   7.73   \n",
      "   1    |   160   |   0.197192   |     -      |     -     |   7.94   \n",
      "   1    |   180   |   0.197894   |     -      |     -     |   9.04   \n",
      "   1    |   200   |   0.183920   |     -      |     -     |   7.84   \n",
      "   1    |   220   |   0.186167   |     -      |     -     |   7.84   \n",
      "   1    |   240   |   0.220119   |     -      |     -     |   7.83   \n",
      "   1    |   260   |   0.168024   |     -      |     -     |   7.87   \n",
      "   1    |   280   |   0.213748   |     -      |     -     |   7.87   \n",
      "   1    |   300   |   0.176257   |     -      |     -     |   7.87   \n",
      "   1    |   320   |   0.214315   |     -      |     -     |   7.85   \n",
      "   1    |   340   |   0.176158   |     -      |     -     |   7.87   \n",
      "   1    |   360   |   0.198761   |     -      |     -     |   7.83   \n",
      "   1    |   380   |   0.179718   |     -      |     -     |   7.90   \n",
      "   1    |   400   |   0.185856   |     -      |     -     |   7.87   \n",
      "   1    |   420   |   0.196853   |     -      |     -     |   7.87   \n",
      "   1    |   440   |   0.189908   |     -      |     -     |   7.85   \n",
      "   1    |   460   |   0.171776   |     -      |     -     |   7.85   \n",
      "   1    |   480   |   0.179236   |     -      |     -     |   7.86   \n",
      "   1    |   490   |   0.187684   |     -      |     -     |   3.79   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.200169   |  0.191675  |   71.36   |  202.71  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 2\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.158060   |     -      |     -     |   9.33   \n",
      "   1    |   40    |   0.171589   |     -      |     -     |   8.65   \n",
      "   1    |   60    |   0.159426   |     -      |     -     |   8.25   \n",
      "   1    |   80    |   0.156590   |     -      |     -     |   8.24   \n",
      "   1    |   100   |   0.156727   |     -      |     -     |   7.84   \n",
      "   1    |   120   |   0.167513   |     -      |     -     |   7.86   \n",
      "   1    |   140   |   0.163836   |     -      |     -     |   7.85   \n",
      "   1    |   160   |   0.140665   |     -      |     -     |   7.85   \n",
      "   1    |   180   |   0.161520   |     -      |     -     |   7.88   \n",
      "   1    |   200   |   0.138523   |     -      |     -     |   7.87   \n",
      "   1    |   220   |   0.183235   |     -      |     -     |   7.88   \n",
      "   1    |   240   |   0.151358   |     -      |     -     |   7.89   \n",
      "   1    |   260   |   0.161386   |     -      |     -     |   7.86   \n",
      "   1    |   280   |   0.160649   |     -      |     -     |   7.90   \n",
      "   1    |   300   |   0.175276   |     -      |     -     |   7.90   \n",
      "   1    |   320   |   0.135578   |     -      |     -     |   7.92   \n",
      "   1    |   340   |   0.164986   |     -      |     -     |   7.92   \n",
      "   1    |   360   |   0.154597   |     -      |     -     |   7.88   \n",
      "   1    |   380   |   0.165048   |     -      |     -     |   7.89   \n",
      "   1    |   400   |   0.146090   |     -      |     -     |   7.89   \n",
      "   1    |   420   |   0.166899   |     -      |     -     |   7.89   \n",
      "   1    |   440   |   0.159687   |     -      |     -     |   7.91   \n",
      "   1    |   460   |   0.166186   |     -      |     -     |   7.88   \n",
      "   1    |   480   |   0.173705   |     -      |     -     |   7.89   \n",
      "   1    |   490   |   0.161692   |     -      |     -     |   3.80   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.159995   |  0.181965  |   73.75   |  203.90  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 3\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.128418   |     -      |     -     |   8.26   \n",
      "   1    |   40    |   0.107219   |     -      |     -     |   8.52   \n",
      "   1    |   60    |   0.111861   |     -      |     -     |   8.50   \n",
      "   1    |   80    |   0.141462   |     -      |     -     |   8.24   \n",
      "   1    |   100   |   0.119684   |     -      |     -     |   8.00   \n",
      "   1    |   120   |   0.133618   |     -      |     -     |   8.04   \n",
      "   1    |   140   |   0.123431   |     -      |     -     |   8.03   \n",
      "   1    |   160   |   0.122320   |     -      |     -     |   8.02   \n",
      "   1    |   180   |   0.138811   |     -      |     -     |   8.01   \n",
      "   1    |   200   |   0.122233   |     -      |     -     |   8.01   \n",
      "   1    |   220   |   0.108083   |     -      |     -     |   7.85   \n",
      "   1    |   240   |   0.128679   |     -      |     -     |   7.66   \n",
      "   1    |   260   |   0.101614   |     -      |     -     |   8.00   \n",
      "   1    |   280   |   0.137877   |     -      |     -     |   8.54   \n",
      "   1    |   300   |   0.136223   |     -      |     -     |   8.06   \n",
      "   1    |   320   |   0.120639   |     -      |     -     |   7.83   \n",
      "   1    |   340   |   0.108534   |     -      |     -     |   7.71   \n",
      "   1    |   360   |   0.135997   |     -      |     -     |   7.72   \n",
      "   1    |   380   |   0.140793   |     -      |     -     |   7.64   \n",
      "   1    |   400   |   0.126213   |     -      |     -     |   7.60   \n",
      "   1    |   420   |   0.110491   |     -      |     -     |   7.52   \n",
      "   1    |   440   |   0.115232   |     -      |     -     |   8.41   \n",
      "   1    |   460   |   0.109603   |     -      |     -     |   8.60   \n",
      "   1    |   480   |   0.138381   |     -      |     -     |   8.35   \n",
      "   1    |   490   |   0.128465   |     -      |     -     |   3.91   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.123750   |  0.193081  |   72.50   |  205.40  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 4\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.078263   |     -      |     -     |   9.23   \n",
      "   1    |   40    |   0.100255   |     -      |     -     |   8.04   \n",
      "   1    |   60    |   0.098917   |     -      |     -     |   8.00   \n",
      "   1    |   80    |   0.101203   |     -      |     -     |   8.02   \n",
      "   1    |   100   |   0.088407   |     -      |     -     |   8.01   \n",
      "   1    |   120   |   0.090750   |     -      |     -     |   8.00   \n",
      "   1    |   140   |   0.086148   |     -      |     -     |   8.00   \n",
      "   1    |   160   |   0.083942   |     -      |     -     |   7.98   \n",
      "   1    |   180   |   0.098588   |     -      |     -     |   7.98   \n",
      "   1    |   200   |   0.101865   |     -      |     -     |   7.98   \n",
      "   1    |   220   |   0.096404   |     -      |     -     |   8.00   \n",
      "   1    |   240   |   0.091863   |     -      |     -     |   7.97   \n",
      "   1    |   260   |   0.062018   |     -      |     -     |   7.98   \n",
      "   1    |   280   |   0.086554   |     -      |     -     |   7.98   \n",
      "   1    |   300   |   0.075349   |     -      |     -     |   7.99   \n",
      "   1    |   320   |   0.081463   |     -      |     -     |   7.99   \n",
      "   1    |   340   |   0.079948   |     -      |     -     |   7.81   \n",
      "   1    |   360   |   0.086535   |     -      |     -     |   7.66   \n",
      "   1    |   380   |   0.080420   |     -      |     -     |   7.59   \n",
      "   1    |   400   |   0.087504   |     -      |     -     |   8.50   \n",
      "   1    |   420   |   0.088657   |     -      |     -     |   8.09   \n",
      "   1    |   440   |   0.094986   |     -      |     -     |   7.95   \n",
      "   1    |   460   |   0.086298   |     -      |     -     |   7.95   \n",
      "   1    |   480   |   0.072707   |     -      |     -     |   7.97   \n",
      "   1    |   490   |   0.085182   |     -      |     -     |   3.84   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.087395   |  0.210815  |   72.61   |  204.61  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 5\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.065501   |     -      |     -     |   8.93   \n",
      "   1    |   40    |   0.058982   |     -      |     -     |   8.73   \n",
      "   1    |   60    |   0.089871   |     -      |     -     |   8.43   \n",
      "   1    |   80    |   0.065411   |     -      |     -     |   7.81   \n",
      "   1    |   100   |   0.055853   |     -      |     -     |   7.83   \n",
      "   1    |   120   |   0.045295   |     -      |     -     |   7.81   \n",
      "   1    |   140   |   0.056274   |     -      |     -     |   7.82   \n",
      "   1    |   160   |   0.071282   |     -      |     -     |   7.81   \n",
      "   1    |   180   |   0.070248   |     -      |     -     |   7.81   \n",
      "   1    |   200   |   0.058714   |     -      |     -     |   7.58   \n",
      "   1    |   220   |   0.066325   |     -      |     -     |   8.39   \n",
      "   1    |   240   |   0.063274   |     -      |     -     |   8.38   \n",
      "   1    |   260   |   0.059115   |     -      |     -     |   8.00   \n",
      "   1    |   280   |   0.049766   |     -      |     -     |   7.90   \n",
      "   1    |   300   |   0.050939   |     -      |     -     |   7.91   \n",
      "   1    |   320   |   0.089806   |     -      |     -     |   7.91   \n",
      "   1    |   340   |   0.080467   |     -      |     -     |   7.91   \n",
      "   1    |   360   |   0.076153   |     -      |     -     |   7.90   \n",
      "   1    |   380   |   0.055928   |     -      |     -     |   7.93   \n",
      "   1    |   400   |   0.059601   |     -      |     -     |   7.93   \n",
      "   1    |   420   |   0.068456   |     -      |     -     |   7.90   \n",
      "   1    |   440   |   0.078454   |     -      |     -     |   7.91   \n",
      "   1    |   460   |   0.052496   |     -      |     -     |   7.92   \n",
      "   1    |   480   |   0.081387   |     -      |     -     |   7.90   \n",
      "   1    |   490   |   0.062107   |     -      |     -     |   3.80   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.065333   |  0.218305  |   71.93   |  204.13  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Fold 4\n",
      "Epoch 1\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.293509   |     -      |     -     |   8.09   \n",
      "   1    |   40    |   0.239986   |     -      |     -     |   8.16   \n",
      "   1    |   60    |   0.245550   |     -      |     -     |   8.17   \n",
      "   1    |   80    |   0.240111   |     -      |     -     |   8.15   \n",
      "   1    |   100   |   0.224934   |     -      |     -     |   8.16   \n",
      "   1    |   120   |   0.206058   |     -      |     -     |   8.08   \n",
      "   1    |   140   |   0.212627   |     -      |     -     |   8.07   \n",
      "   1    |   160   |   0.194279   |     -      |     -     |   8.08   \n",
      "   1    |   180   |   0.181436   |     -      |     -     |   7.82   \n",
      "   1    |   200   |   0.181939   |     -      |     -     |   7.77   \n",
      "   1    |   220   |   0.174034   |     -      |     -     |   7.77   \n",
      "   1    |   240   |   0.197867   |     -      |     -     |   7.76   \n",
      "   1    |   260   |   0.204912   |     -      |     -     |   7.79   \n",
      "   1    |   280   |   0.213536   |     -      |     -     |   7.88   \n",
      "   1    |   300   |   0.199368   |     -      |     -     |   8.21   \n",
      "   1    |   320   |   0.210947   |     -      |     -     |   8.26   \n",
      "   1    |   340   |   0.190487   |     -      |     -     |   8.24   \n",
      "   1    |   360   |   0.199481   |     -      |     -     |   8.24   \n",
      "   1    |   380   |   0.168736   |     -      |     -     |   8.22   \n",
      "   1    |   400   |   0.176991   |     -      |     -     |   8.20   \n",
      "   1    |   420   |   0.189168   |     -      |     -     |   7.64   \n",
      "   1    |   440   |   0.217228   |     -      |     -     |   7.52   \n",
      "   1    |   460   |   0.208809   |     -      |     -     |   7.42   \n",
      "   1    |   480   |   0.178833   |     -      |     -     |   7.68   \n",
      "   1    |   490   |   0.158968   |     -      |     -     |   3.80   \n",
      "----------------------------------------------------------------------\n",
      "   1    |    -    |   0.205498   |  0.166124  |   76.59   |  203.10  \n",
      "----------------------------------------------------------------------\n",
      "\n",
      "\n",
      "Training complete!\n",
      "Epoch 2\n",
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val Acc  |  Elapsed \n",
      "----------------------------------------------------------------------\n",
      "   1    |   20    |   0.171808   |     -      |     -     |   9.02   \n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a directory to save the results\n",
    "rootdir = \"LRADAMWDROPOUT_distilbert_fold_results\"\n",
    "if not os.path.exists(\"%s\" % rootdir):\n",
    "    os.mkdir(rootdir)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "batch_size = 16# Loop over each fold\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "    print(f\"Fold {fold}\")\n",
    "    fold_metrics = []\n",
    "    probsandlab = [['Probability', 'Label']]\n",
    "    # Create a directory for the current fold\n",
    "    fold_dir = f\"%s/fold_{fold}\" % rootdir\n",
    "    if not os.path.exists(fold_dir):\n",
    "        os.mkdir(fold_dir)\n",
    "\n",
    "    # Create the DataLoader for our training set\n",
    "    train_data = TensorDataset(X_inputs[train_idx], X_masks[train_idx], y[train_idx])\n",
    "    train_sampler = RandomSampler(train_data) # No adjust weight\n",
    "    # train_sampler = train_subsampler # Adjusted weight\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create the DataLoader for our validation set\n",
    "    val_data = TensorDataset(X_inputs[val_idx], X_masks[val_idx], y[val_idx])\n",
    "    val_sampler = SequentialSampler(val_data)\n",
    "    val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "    # Create a directory to save the fold's epoch results\n",
    "    epoch_results_dir = f\"{fold_dir}/epoch_results\"\n",
    "    if not os.path.exists(epoch_results_dir):\n",
    "        os.mkdir(epoch_results_dir)\n",
    "\n",
    "    set_seed(42)    # Set seed for reproducibility\n",
    "    bert_classifier, optimizer, scheduler = initialize_model(epochs=2)\n",
    "\n",
    "    # Loop over epochs\n",
    "    for epoch in range(1, 3):\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        train(bert_classifier, train_dataloader, val_dataloader, epochs=1, evaluation=True)\n",
    "\n",
    "        # Save the model\n",
    "        model_path = f\"{epoch_results_dir}/model_epoch_{epoch}.pt\"\n",
    "        torch.save(bert_classifier.state_dict(), model_path)\n",
    "\n",
    "        # Compute predicted probabilities on the validation set\n",
    "        val_probs = bert_predict(bert_classifier, val_dataloader)\n",
    "        val_preds = (val_probs > 0.5).astype(int)\n",
    "        val_labels = y[val_idx].detach().numpy()\n",
    "\n",
    "        # Convert val_probs and val_labels to lists\n",
    "        val_probs_list = val_probs.tolist()\n",
    "        val_labels_list = val_labels.tolist()\n",
    "\n",
    "        # Prepare the data for writing to CSV\n",
    "        probsandlab.extend(list(zip(val_probs_list, val_labels_list)))\n",
    "\n",
    "        # Create a DataFrame for the fold's metrics\n",
    "        fold_metrics.append({\n",
    "            'epoch': epoch,\n",
    "            'predicted_probs': val_probs,\n",
    "            'predicted_label': val_preds,\n",
    "            'true_label': val_labels,\n",
    "            'accuracy': accuracy_score(val_labels, val_preds),\n",
    "            'precision': precision_score(val_labels, val_preds),\n",
    "            'recall': recall_score(val_labels, val_preds),\n",
    "            'f1': f1_score(val_labels, val_preds),\n",
    "        })\n",
    "\n",
    "        # Save the classification report to a text file\n",
    "        report_path = f\"{epoch_results_dir}/classification_report_epoch_{epoch}.txt\"\n",
    "        with open(report_path, 'w') as report_file:\n",
    "            report_file.write(classification_report(val_labels, val_preds,digits=6))\n",
    "\n",
    "    metricss_path = f\"{fold_dir}/probsandlab_{fold}_bert.csv\"\n",
    "    probsandlab_df = pd.DataFrame(probsandlab[1:], columns=probsandlab[0])\n",
    "    probsandlab_df.to_csv(metricss_path, index=False)\n",
    "\n",
    "# Create a DataFrame for the fold's metrics\n",
    "    fold_df = pd.DataFrame(fold_metrics)\n",
    "\n",
    "    # Save the fold's metrics to a CSV file\n",
    "    metrics_path = f\"{fold_dir}/fold_{fold}_bert.csv\"\n",
    "    fold_df.to_csv(metrics_path, index=False)\n",
    "\n",
    "    # Plot the validation accuracy over the number of epochs\n",
    "    plt.plot(fold_df['epoch'], fold_df['accuracy'])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Validation Accuracy')\n",
    "    plt.title(f'Fold {fold} - Validation Accuracy')\n",
    "    plt.savefig(f\"{fold_dir}/fold_{fold}_accuracy_plot.jpg\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
